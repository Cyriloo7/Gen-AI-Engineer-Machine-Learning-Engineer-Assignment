{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGVHTJP_j4MH"
      },
      "source": [
        "# Part 1: Retrieval-Augmented Generation (RAG) Model for QA Bot\n",
        "\n",
        "### Problem Statement:\n",
        "\n",
        "### Develop a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA)\n",
        "\n",
        "### bot for a business. Use a vector database like Pinecone DB and a generative model like\n",
        "\n",
        "### Cohere API (or any other available alternative). The QA bot should be able to retrieve\n",
        "\n",
        "### relevant information from a dataset and generate coherent answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu cohere PyPDF2 numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyvYUYiT8zP6",
        "outputId": "0b270aae-4159-439f-c7ac-6e4533fc59a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting cohere\n",
            "  Downloading cohere-5.9.2-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Collecting boto3<2.0.0,>=1.34.0 (from cohere)\n",
            "  Downloading boto3-1.35.19-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx>=0.21.2 (from cohere)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.9.1)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.23.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.19.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20240914-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.12.2)\n",
            "Collecting botocore<1.36.0,>=1.35.19 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading botocore-1.35.19-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.21.2->cohere)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.21.2->cohere)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.0.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15->cohere) (0.24.6)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.19->boto3<2.0.0,>=1.34.0->cohere) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.66.5)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.19->boto3<2.0.0,>=1.34.0->cohere) (1.16.0)\n",
            "Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.9.2-py3-none-any.whl (222 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.4/222.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.35.19-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading types_requests-2.32.0.20240914-py3-none-any.whl (15 kB)\n",
            "Downloading botocore-1.35.19-py3-none-any.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: types-requests, PyPDF2, parameterized, jmespath, httpx-sse, h11, fastavro, faiss-cpu, httpcore, botocore, s3transfer, httpx, boto3, cohere\n",
            "Successfully installed PyPDF2-3.0.1 boto3-1.35.19 botocore-1.35.19 cohere-5.9.2 faiss-cpu-1.8.0.post1 fastavro-1.9.7 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 httpx-sse-0.4.0 jmespath-1.0.1 parameterized-0.9.0 s3transfer-0.10.2 types-requests-2.32.0.20240914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "N1Ag3mUJ80Zi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text, chunk_size=1000):\n",
        "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "AyP9OUzn864r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "source": [
        "import cohere\n",
        "import numpy as np\n",
        "\n",
        "co = cohere.Client('SGXUJ2vUDqaNNpJwh1ffmo1PFkGmN50W6ghcW4UA')  # Replace with your Cohere API key\n",
        "\n",
        "def create_embeddings(texts, batch_size=40):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "\n",
        "        batch = texts[i:i+batch_size]\n",
        "        # Added input_type argument for the embed-english-v3.0 model\n",
        "        response = co.embed(texts=batch, model=\"embed-english-v3.0\", input_type=\"search_document\")\n",
        "        embeddings.append(response.embeddings)\n",
        "\n",
        "    return np.vstack(embeddings)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "U8MqPnoi-zeI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Define index parameters\n",
        "dimension = 1024  # Cohere's embedding model dimensionality\n",
        "index = faiss.IndexFlatL2(dimension)  # FAISS L2 (cosine) index\n"
      ],
      "metadata": {
        "id": "vAswz5XG9Muz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example PDF path\n",
        "pdf_path = '/content/Gen AI Engineer _ Machine Learning Engineer Assignment.pdf'\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "chunks = split_text(text)\n",
        "\n",
        "# Generate embeddings\n",
        "chunk_embeddings = create_embeddings(chunks)\n",
        "\n",
        "# Add embeddings to FAISS index\n",
        "index.add(np.array(chunk_embeddings).astype(np.float32))\n"
      ],
      "metadata": {
        "id": "JoE3O3SH9QAC"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, index, k=3):\n",
        "    # Create query embedding with Cohere\n",
        "    query_embed = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_document\").embeddings\n",
        "    # Search in FAISS index\n",
        "    # Convert query_embed to a 1D array before passing it to index.search\n",
        "    D, I = index.search(np.array(query_embed).astype(np.float32), k)\n",
        "\n",
        "    # Fetch relevant documents\n",
        "    return [chunks[i] for i in I[0]]"
      ],
      "metadata": {
        "id": "d44tq_me9lpR"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the main topic of the document? Give a summary.\"\n",
        "context = retrieve(query, index)\n",
        "\n",
        "contexts = \"\"\n",
        "for cont in context:\n",
        "  contexts = contexts + cont\n",
        "\n"
      ],
      "metadata": {
        "id": "uBDw4ND-9n5Z"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "\n",
        "stream = co.chat_stream(\n",
        "  model='command-r-plus-08-2024',\n",
        "  message=contexts,\n",
        "  temperature=0.4,\n",
        "  chat_history=[],\n",
        "  prompt_truncation='AUTO',\n",
        "  #connectors=[{\"id\":\"web-search\"}],\n",
        "  max_tokens=4096\n",
        ")\n",
        "\n",
        "for event in stream:\n",
        "  if event.event_type == \"text-generation\":\n",
        "    print(event.text, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "annIc2CgB1od",
        "outputId": "c6735b71-4ae3-4e33-e64d-6a4c9f27510f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## I. Introduction:\n",
            "\n",
            "In the era of rapidly evolving digital manipulation techniques, the need for robust deepfake detection methods has never been more critical. Deepfakes, a portmanteau of \"deep learning\" and \"fake,\" pose significant challenges to various sectors, including media, politics, and personal privacy. This paper introduces a groundbreaking deepfake detection system that integrates both visual and auditory cues, marking a significant advancement in the field.\n",
            "\n",
            "## II. Literature Review:\n",
            "\n",
            "Recent research has made notable strides in the battle against deepfakes. Yu et al. (2023) presented a compelling approach by combining EfficientNet's efficient feature extraction capabilities with torchvision, demonstrating the potential of innovative techniques. However, our proposed method takes this a step further by incorporating both visual and auditory analysis.\n",
            "\n",
            "## VI. Feature Extraction:\n",
            "\n",
            "The process of feature extraction is pivotal in machine learning, and this code employs two cutting-edge transformer models for this purpose: the Vision Transformer (ViT) and Wav2Vec2.\n",
            "\n",
            "### A. Vision Transformer (ViT):\n",
            "The ViT model, pre-trained on the ImageNet dataset, is utilized for video data processing. Its ability to capture intricate details in each frame, from facial expressions to lighting nuances, is pivotal in detecting visual anomalies indicative of deepfakes.\n",
            "\n",
            "### B. Wav2Vec2 Transformer:\n",
            "Simultaneously, the Wav2Vec2 model, introduced by Baevski et al. (2020), is employed for audio feature extraction. This model has proven its prowess in speech recognition tasks and is adept at identifying subtle variations in speech patterns and voice characteristics, making it invaluable for detecting audio manipulations.\n",
            "\n",
            "After extracting features from both modalities, they are concatenated and fed into an attention layer, which prioritizes the most salient features. Subsequently, a classification layer predicts the class of the input data, distinguishing between authentic and deepfake content.\n",
            "\n",
            "## VII. Methodology:\n",
            "\n",
            "The proposed architecture is a harmonious fusion of visual and auditory analysis, ensuring a comprehensive evaluation of multimedia content.\n",
            "\n",
            "### A. Data Collection and Preprocessing:\n",
            "The dataset for this study comprises authentic videos and their corresponding deepfake versions, generated using state-of-the-art deepfake creation techniques. Preprocessing steps include resizing video frames and normalizing audio data, ensuring consistency and compatibility with the chosen models.\n",
            "\n",
            "### B. Model Architecture:\n",
            "The model consists of two primary components: the ViT for visual analysis and the Wav2Vec2 for audio processing. The ViT captures visual features, while the Wav2Vec2 extracts audio embeddings. These features are then concatenated and passed through an attention mechanism, allowing the model to weigh the importance of different features. Finally, a fully connected layer with a sigmoid activation function predicts the probability of a video being a deepfake.\n",
            "\n",
            "### C. Training and Evaluation:\n",
            "The model is trained using a binary cross-entropy loss function, optimizing its ability to distinguish between real and fake videos. Evaluation metrics include precision, recall, and F1-score, ensuring a comprehensive assessment of the model's performance.\n",
            "\n",
            "## VIII. Results and Discussion:\n",
            "\n",
            "The proposed deepfake detection system demonstrates exceptional performance, achieving [insert evaluation metrics results] on the test dataset. This approach significantly outperforms existing methods, particularly in detecting subtle deepfakes that often evade traditional detection techniques.\n",
            "\n",
            "The integration of visual and auditory analysis provides a more holistic understanding of multimedia content, making it harder for deepfakes to go unnoticed. The attention mechanism's ability to focus on critical features further enhances the model's effectiveness.\n",
            "\n",
            "## IX. Conclusion and Future Work:\n",
            "\n",
            "In conclusion, this research introduces a novel deepfake detection approach that leverages the strengths of both Vision Transformers and Wav2Vec2 Transformers. By analyzing visual and auditory cues simultaneously, the model achieves remarkable accuracy in identifying deepfakes.\n",
            "\n",
            "Future research directions include exploring more advanced fusion techniques for visual and auditory features, investigating the impact of different attention mechanisms, and extending the model to handle various deepfake generation methods. As deepfake technology evolves, so must our detection strategies, ensuring a safer and more trustworthy digital environment.\n",
            "\n",
            "## References:\n",
            "\n",
            "[1] Yu, et al. (2023). A novel deepfake detection approach using EfficientNet and torchvision. *Journal of Computer Vision and Image Understanding*.\n",
            "\n",
            "[2] Baevski, A., et al. (2020). Wav2Vec 2.0: A framework for self-supervised learning of speech representations. *Advances in Neural Information Processing Systems*.\n",
            "\n",
            "[3] Dosovitskiy, A., et al. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. *International Conference on Learning Representations*.\n",
            "\n",
            "[4] Rossler, A., et al. (2019). FaceForensics++: Learning to detect manipulated facial images. *IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*.\n",
            "\n",
            "[5] Rossler, Adrian, et al. \"Deepfakeforensics: Leveraging deep learning to detect manipulated videos.\" *arXiv preprint arXiv:1901.08410 (2019)*.\n",
            "\n",
            "[6] Ivanov, N. S., et al. (2020). \"Combining deep learning and super-resolution algorithms for deep fake detection.\" *2020 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus)*.\n",
            "\n",
            "[7] Singh, A., Saimbhi, A., Singh, N., et al. \"DeepFake Video Detection: A Time-Distributed Approach.\"\n",
            "\n",
            "[8] Doke, Yash. \"Deep Fake Detection Through Deep Learning.\" *Unpublished manuscript, 2023*.\n",
            "\n",
            "[9] Suratkar, Ganesh, and Aijaz Kazi. \"Deep Fake Video Detection Using Transfer Learning Approach.\" *2022 International Conference on Machine Learning and Computing (ICMLC), IEEE, 2022*."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}