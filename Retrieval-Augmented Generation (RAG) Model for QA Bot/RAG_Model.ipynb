{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGVHTJP_j4MH"
      },
      "source": [
        "# Part 1: Retrieval-Augmented Generation (RAG) Model for QA Bot\n",
        "\n",
        "### Problem Statement:\n",
        "\n",
        "### Develop a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA)\n",
        "\n",
        "### bot for a business. Use a vector database like Pinecone DB and a generative model like\n",
        "\n",
        "### Cohere API (or any other available alternative). The QA bot should be able to retrieve\n",
        "\n",
        "### relevant information from a dataset and generate coherent answers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv_IQZ_KIik9"
      },
      "source": [
        "## Installation Requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyvYUYiT8zP6",
        "outputId": "0b270aae-4159-439f-c7ac-6e4533fc59a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting cohere\n",
            "  Downloading cohere-5.9.2-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Collecting boto3<2.0.0,>=1.34.0 (from cohere)\n",
            "  Downloading boto3-1.35.19-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx>=0.21.2 (from cohere)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.9.1)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.23.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.19.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20240914-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.12.2)\n",
            "Collecting botocore<1.36.0,>=1.35.19 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading botocore-1.35.19-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.21.2->cohere)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.21.2->cohere)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.0.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15->cohere) (0.24.6)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.19->boto3<2.0.0,>=1.34.0->cohere) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.66.5)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.19->boto3<2.0.0,>=1.34.0->cohere) (1.16.0)\n",
            "Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.9.2-py3-none-any.whl (222 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.4/222.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.35.19-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading types_requests-2.32.0.20240914-py3-none-any.whl (15 kB)\n",
            "Downloading botocore-1.35.19-py3-none-any.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: types-requests, PyPDF2, parameterized, jmespath, httpx-sse, h11, fastavro, faiss-cpu, httpcore, botocore, s3transfer, httpx, boto3, cohere\n",
            "Successfully installed PyPDF2-3.0.1 boto3-1.35.19 botocore-1.35.19 cohere-5.9.2 faiss-cpu-1.8.0.post1 fastavro-1.9.7 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 httpx-sse-0.4.0 jmespath-1.0.1 parameterized-0.9.0 s3transfer-0.10.2 types-requests-2.32.0.20240914\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu cohere PyPDF2 numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Package                            Version\n",
            "---------------------------------- ---------------\n",
            "accelerate                         0.33.0\n",
            "aiohttp                            3.9.5\n",
            "aiohttp-retry                      2.8.3\n",
            "aiosignal                          1.3.1\n",
            "alembic                            1.13.2\n",
            "altair                             5.4.1\n",
            "amqp                               5.2.0\n",
            "aniso8601                          9.0.1\n",
            "annotated-types                    0.7.0\n",
            "antlr4-python3-runtime             4.9.3\n",
            "anyio                              3.7.1\n",
            "anyio                              3.7.1\n",
            "appdirs                            1.4.4\n",
            "asttokens                          2.4.1\n",
            "asyncssh                           2.15.0\n",
            "atpublic                           5.0\n",
            "attrs                              23.2.0\n",
            "autocommand                        2.2.2\n",
            "backoff                            2.2.1\n",
            "backports.tarfile                  1.2.0\n",
            "beautifulsoup4                     4.12.3\n",
            "billiard                           4.2.0\n",
            "bleach                             6.1.0\n",
            "blinker                            1.8.2\n",
            "boto3                              1.34.149\n",
            "botocore                           1.34.149\n",
            "cachetools                         5.4.0\n",
            "celery                             5.4.0\n",
            "certifi                            2024.7.4\n",
            "cffi                               1.16.0\n",
            "chardet                            3.0.4\n",
            "charset-normalizer                 3.3.2\n",
            "click                              8.1.7\n",
            "click-didyoumean                   0.3.1\n",
            "click-plugins                      1.1.1\n",
            "click-repl                         0.3.0\n",
            "cloudpickle                        3.0.0\n",
            "cohere                             5.9.2\n",
            "cohere                             5.9.2\n",
            "colorama                           0.4.6\n",
            "comm                               0.2.2\n",
            "configobj                          5.0.8\n",
            "contourpy                          1.2.1\n",
            "cryptography                       43.0.0\n",
            "cycler                             0.12.1\n",
            "datasets                           2.20.0\n",
            "debugpy                            1.8.2\n",
            "decorator                          5.1.1\n",
            "Deprecated                         1.2.14\n",
            "dictdiffer                         0.9.0\n",
            "diffusers                          0.29.2\n",
            "dill                               0.3.8\n",
            "diskcache                          5.6.3\n",
            "distro                             1.9.0\n",
            "docker                             7.1.0\n",
            "dpath                              2.2.0\n",
            "dulwich                            0.22.1\n",
            "dvc                                3.52.0\n",
            "dvc-data                           3.15.1\n",
            "dvc-http                           2.32.0\n",
            "dvc-objects                        5.1.0\n",
            "dvc-render                         1.0.2\n",
            "dvc-studio-client                  0.21.0\n",
            "dvc-task                           0.4.0\n",
            "entrypoints                        0.4\n",
            "evaluate                           0.4.2\n",
            "executing                          2.0.1\n",
            "faiss-cpu                          1.8.0.post1\n",
            "fastavro                           1.9.7\n",
            "fastavro                           1.9.7\n",
            "filelock                           3.15.4\n",
            "flaky                              3.8.1\n",
            "Flask                              3.0.3\n",
            "flatten-dict                       0.4.2\n",
            "flufl.lock                         8.1.0\n",
            "fonttools                          4.53.1\n",
            "frozenlist                         1.4.1\n",
            "fsspec                             2024.5.0\n",
            "funcy                              2.0\n",
            "gitdb                              4.0.11\n",
            "GitPython                          3.1.43\n",
            "google-api-core                    2.19.1\n",
            "google-auth                        2.32.0\n",
            "google-cloud-automl                2.11.1\n",
            "google-cloud-core                  2.4.1\n",
            "google-cloud-storage               2.9.0\n",
            "google-cloud-translate             3.11.1\n",
            "google-crc32c                      1.5.0\n",
            "google-resumable-media             2.7.1\n",
            "googleapis-common-protos           1.63.2\n",
            "grandalf                           0.8\n",
            "graphene                           3.3\n",
            "graphql-core                       3.2.3\n",
            "graphql-relay                      3.2.0\n",
            "greenlet                           3.0.3\n",
            "grpcio                             1.65.1\n",
            "grpcio-status                      1.62.2\n",
            "gto                                1.7.1\n",
            "h11                                0.14.0\n",
            "h11                                0.12.0\n",
            "h2                                 3.2.0\n",
            "hpack                              3.0.0\n",
            "hstspreload                        2024.7.1\n",
            "httpcore                           1.0.5\n",
            "httpcore                           0.13.7\n",
            "httpx                              0.27.2\n",
            "httpx-sse                          0.4.0\n",
            "httpx-sse                          0.4.0\n",
            "huggingface-hub                    0.24.0\n",
            "hydra-core                         1.3.2\n",
            "hyperframe                         5.2.0\n",
            "idna                               2.10\n",
            "importlib_metadata                 7.2.1\n",
            "importlib_resources                6.4.0\n",
            "inflect                            7.3.1\n",
            "inflection                         0.5.1\n",
            "iniconfig                          2.0.0\n",
            "intel-openmp                       2021.4.0\n",
            "ipykernel                          6.29.5\n",
            "ipython                            8.26.0\n",
            "iterative-telemetry                0.0.8\n",
            "itsdangerous                       2.2.0\n",
            "jaraco.context                     5.3.0\n",
            "jaraco.functools                   4.0.1\n",
            "jaraco.text                        3.12.1\n",
            "jedi                               0.19.1\n",
            "Jinja2                             3.1.4\n",
            "jmespath                           1.0.1\n",
            "joblib                             1.4.2\n",
            "jsonschema                         4.23.0\n",
            "jsonschema-specifications          2023.12.1\n",
            "jupyter_client                     8.6.2\n",
            "jupyter_core                       5.7.2\n",
            "kaggle                             1.6.14\n",
            "kiwisolver                         1.4.5\n",
            "kombu                              5.3.7\n",
            "langdetect                         1.0.9\n",
            "lyricsgenius                       3.0.1\n",
            "Mako                               1.3.5\n",
            "Markdown                           3.6\n",
            "markdown-it-py                     3.0.0\n",
            "MarkupSafe                         2.1.5\n",
            "matplotlib                         3.9.1\n",
            "matplotlib-inline                  0.1.7\n",
            "mdurl                              0.1.2\n",
            "mkl                                2021.4.0\n",
            "mlflow                             2.14.3\n",
            "more-itertools                     10.3.0\n",
            "mpmath                             1.3.0\n",
            "multidict                          6.0.5\n",
            "multiprocess                       0.70.16\n",
            "narwhals                           1.8.1\n",
            "nest-asyncio                       1.6.0\n",
            "networkx                           3.3\n",
            "nltk                               3.8.1\n",
            "numpy                              1.26.4\n",
            "omegaconf                          2.3.0\n",
            "opencv-python                      4.10.0.84\n",
            "opentelemetry-api                  1.26.0\n",
            "opentelemetry-sdk                  1.26.0\n",
            "opentelemetry-semantic-conventions 0.47b0\n",
            "ordered-set                        4.1.0\n",
            "orjson                             3.10.6\n",
            "packaging                          24.1\n",
            "pandas                             2.2.2\n",
            "pandas                             2.2.2\n",
            "parameterized                      0.9.0\n",
            "parameterized                      0.9.0\n",
            "parso                              0.8.4\n",
            "pathspec                           0.12.1\n",
            "pillow                             10.4.0\n",
            "pinecone                           5.1.0\n",
            "pinecone-plugin-inference          1.0.3\n",
            "pinecone-plugin-interface          0.0.7\n",
            "pip                                24.2\n",
            "pipdeptree                         2.23.1\n",
            "platformdirs                       3.11.0\n",
            "pluggy                             1.5.0\n",
            "prettier                           0.0.7\n",
            "profanity-check                    1.0.3\n",
            "profanityfilter                    2.0.6\n",
            "prompt_toolkit                     3.0.47\n",
            "proto-plus                         1.24.0\n",
            "protobuf                           4.25.3\n",
            "psutil                             6.0.0\n",
            "pure_eval                          0.2.3\n",
            "pyarrow                            15.0.2\n",
            "pyarrow-hotfix                     0.6\n",
            "pyasn1                             0.6.0\n",
            "pyasn1_modules                     0.4.0\n",
            "pycparser                          2.22\n",
            "pydantic                           2.8.2\n",
            "pydantic_core                      2.20.1\n",
            "pydeck                             0.9.1\n",
            "pydot                              3.0.1\n",
            "pygit2                             1.15.1\n",
            "Pygments                           2.18.0\n",
            "pygtrie                            2.5.0\n",
            "pyparsing                          3.1.2\n",
            "PyPDF2                             3.0.1\n",
            "pytest                             8.2.0\n",
            "python-dateutil                    2.9.0.post0\n",
            "python-dateutil                    2.9.0.post0\n",
            "python-slugify                     8.0.4\n",
            "pytz                               2024.1\n",
            "pytz                               2024.1\n",
            "pywin32                            306\n",
            "PyYAML                             6.0.1\n",
            "pyzmq                              26.0.3\n",
            "QA-Application                     0.0.1\n",
            "QA-Application                     0.0.1\n",
            "QA-Application                     0.0.1\n",
            "querystring-parser                 1.2.4\n",
            "redis                              5.0.7\n",
            "referencing                        0.35.1\n",
            "regex                              2024.5.15\n",
            "requests                           2.32.3\n",
            "rfc3986                            1.5.0\n",
            "rich                               13.7.1\n",
            "rpds-py                            0.20.0\n",
            "rsa                                4.9\n",
            "ruamel.yaml                        0.18.6\n",
            "ruamel.yaml.clib                   0.2.8\n",
            "s3transfer                         0.10.2\n",
            "safetensors                        0.4.3\n",
            "scikit-learn                       1.5.0\n",
            "scipy                              1.14.0\n",
            "scmrepo                            3.3.6\n",
            "semver                             3.0.2\n",
            "sentence-transformers              3.1.0\n",
            "setuptools                         71.1.0\n",
            "shellingham                        1.5.4\n",
            "shortuuid                          1.0.13\n",
            "shtab                              1.7.1\n",
            "six                                1.16.0\n",
            "six                                1.16.0\n",
            "smmap                              5.0.1\n",
            "sniffio                            1.3.1\n",
            "soupsieve                          2.5\n",
            "spotipy                            2.24.0\n",
            "SQLAlchemy                         2.0.31\n",
            "sqlparse                           0.5.1\n",
            "sqltrie                            0.11.0\n",
            "stack-data                         0.6.3\n",
            "streamlit                          1.38.0\n",
            "sympy                              1.13.1\n",
            "tabulate                           0.9.0\n",
            "tbb                                2021.13.0\n",
            "tenacity                           8.5.0\n",
            "text-unidecode                     1.3\n",
            "threadpoolctl                      3.5.0\n",
            "tokenizers                         0.19.1\n",
            "toml                               0.10.2\n",
            "tomli                              2.0.1\n",
            "tomlkit                            0.13.0\n",
            "torch                              2.3.1+cu121\n",
            "torchaudio                         2.3.1+cu121\n",
            "torchvision                        0.18.1\n",
            "tornado                            6.4.1\n",
            "tqdm                               4.66.4\n",
            "traitlets                          5.14.3\n",
            "transformers                       4.42.4\n",
            "typeguard                          4.3.0\n",
            "typer                              0.12.3\n",
            "types-requests                     2.32.0.20240914\n",
            "types-requests                     2.32.0.20240914\n",
            "typing_extensions                  4.12.2\n",
            "tzdata                             2024.1\n",
            "tzdata                             2024.1\n",
            "urllib3                            2.2.2\n",
            "vine                               5.1.0\n",
            "voluptuous                         0.15.2\n",
            "waitress                           3.0.0\n",
            "watchdog                           4.0.2\n",
            "wcwidth                            0.2.13\n",
            "webencodings                       0.5.1\n",
            "Werkzeug                           3.0.3\n",
            "wheel                              0.43.0\n",
            "wrapt                              1.16.0\n",
            "xxhash                             3.4.1\n",
            "yarl                               1.9.4\n",
            "zc.lockfile                        3.0.post1\n",
            "zipp                               3.19.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEPRECATION: Loading egg at c:\\users\\cyril\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\anyio-3.7.1-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\users\\cyril\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\cohere-5.9.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\users\\cyril\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\fastavro-1.9.7-py3.12-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\users\\cyril\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\h11-0.12.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\users\\cyril\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\httpcore-0.13.7-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\users\\cyril\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\httpx_sse-0.4.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\users\\cyril\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\pandas-2.2.2-py3.12-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\users\\cyril\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\parameterized-0.9.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\users\\cyril\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\python_dateutil-2.9.0.post0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\users\\cyril\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\pytz-2024.1-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\users\\cyril\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\qa_application-0.0.1-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\users\\cyril\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\six-1.16.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\users\\cyril\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\types_requests-2.32.0.20240914-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\users\\cyril\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\tzdata-2024.1-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
          ]
        }
      ],
      "source": [
        "!pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall httpx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB5yWxwqIhI0"
      },
      "source": [
        "## Read contexts from PDF file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "N1Ag3mUJ80Zi"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBJKm-BRIt-1"
      },
      "source": [
        "## Spliting the context into chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AyP9OUzn864r"
      },
      "outputs": [],
      "source": [
        "def split_text(text, chunk_size=1000):\n",
        "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLGsH7d5KqMS"
      },
      "source": [
        "## Using Cohere API services for Text Embeding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "U8MqPnoi-zeI"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "import numpy as np\n",
        "\n",
        "co = cohere.Client('SGXUJ2vUDqaNNpJwh1ffmo1PFkGmN50W6ghcW4UA')\n",
        "\n",
        "def create_embeddings(texts, batch_size=40):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "\n",
        "        batch = texts[i:i+batch_size]\n",
        "        print(batch)\n",
        "        response = co.embed(texts=batch, model=\"embed-english-v3.0\", input_type=\"search_document\")\n",
        "        embeddings.append(response.embeddings)\n",
        "\n",
        "    return np.vstack(embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB5T_i-YLJvi"
      },
      "source": [
        "## Initializing FAISS Indexing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "vAswz5XG9Muz"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "dimension = 1024\n",
        "index = faiss.IndexFlatL2(dimension)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWQb3orCLPmS"
      },
      "source": [
        "## Processing: Reading a pdf file, Extracting text from that PDF, Spliting into chunks of data, Vector Embeding and Converting the embeded data into FAISS Index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "JoE3O3SH9QAC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Applied \\nGenerative AI for Beginners\\nPractical Knowledge on Diffusion Models, \\nChatGPT, and Other LLMs\\n—\\nAkshay Kulkarni\\nAdarsha ShivanandaAnoosh KulkarniDilip GudivadaApplied Generative AI for \\nBeginners\\nPractical Knowledge on\\xa0Diffusion \\nModels, ChatGPT, and\\xa0Other LLMs\\nAkshay\\xa0Kulkarni\\nAdarsha\\xa0Shivananda\\nAnoosh\\xa0Kulkarni\\nDilip\\xa0GudivadaApplied Generative AI for Beginners: Practical Knowledge on Diffusion Models, \\nChatGPT, and Other LLMs\\nISBN-13 (pbk): 978-1-4842-9993-7   ISBN-13 (electronic): 978-1-4842-9994-4\\nhttps://doi.org/10.1007/978-1-4842-9994-4\\nCopyright © 2023 by Akshay Kulkarni, Adarsha Shivananda, Anoosh Kulkarni,  \\nDilip Gudivada\\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the \\nmaterial is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, \\nbroadcasting, reproduction on microfilms or in any other physical way, and transmission or information \\nstorage and retrieval, ele', 'ctronic adaptation, computer software, or by similar or dissimilar methodology now \\nknown or hereafter developed.\\nTrademarked names, logos, and images may appear in this book. Rather than use a trademark symbol with \\nevery occurrence of a trademarked name, logo, or image we use the names, logos, and images only in an \\neditorial fashion and to the benefit of the trademark owner, with no intention of infringement of the \\ntrademark.\\nThe use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not \\nidentified as such, is not to be taken as an expression of opinion as to whether or not they are subject to \\nproprietary rights.\\nWhile the advice and information in this book are believed to be true and accurate at the date of publication, \\nneither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or \\nomissions that may be made. The publisher makes no warranty, express or implied, with respect to the \\nma', 'terial contained herein.\\nManaging Director, Apress Media LLC: Welmoed Spahr\\nAcquisitions Editor: Celestin Suresh John\\nDevelopment Editor: Laura Berendson\\nEditorial Assistant: Gryffin Winkler\\nCover designed by eStudioCalamar\\nCover image designed by Scott Webb on unsplash\\nDistributed to the book trade worldwide by Springer Science+Business Media New\\xa0York, 1 New\\xa0York Plaza, \\nSuite 4600, New\\xa0York, NY 10004-1562, USA.\\xa0Phone 1-800-SPRINGER, fax (201) 348-4505, e-mail orders-ny@\\nspringer-sbm.com, or visit www.springeronline.com. Apress Media, LLC is a California LLC and the sole \\nmember (owner) is Springer Science + Business Media Finance Inc (SSBM Finance Inc). SSBM Finance Inc \\nis a Delaware  corporation.\\nFor information on translations, please e-mail booktranslations@springernature.com; for reprint, \\npaperback, or audio rights, please e-mail bookpermissions@springernature.com.\\nApress titles may be purchased in bulk for academic, corporate, or promotional use. eBook versions and \\nlicenses a', 're also available for most titles. For more information, reference our Print and eBook Bulk Sales \\nweb page at http://www.apress.com/bulk-sales.\\nAny source code or other supplementary material referenced by the author in this book is available to \\nreaders on GitHub. For more detailed information, please visit https://www.apress.com/gp/services/\\nsource-code.\\nPaper in this product is recyclableAkshay\\xa0Kulkarni\\nBangalore, Karnataka, India\\nAdarsha\\xa0Shivananda\\nHosanagara, Karnataka, IndiaAnoosh\\xa0Kulkarni\\nBangalore, Karnataka, India\\nDilip\\xa0Gudivada\\nBangalore, IndiaTo our familiesvTable of Contents\\nAbout the Authors  ���������������������������������������������������������������������������������������������������� xi\\nAbout the Technical Reviewer  ������������������������������������������������������������������������������� xiii\\nIntroduction  ������������������������������������������������������������������������������������������������������������� xv\\nChapter 1:   Introduction to Generative ', 'AI  ������������������������������������������������������������������ 1\\nSo, What Is Generative AI?  ������������������������������������������������������������������������������������������������������������ 2\\nComponents of AI  �������������������������������������������������������������������������������������������������������������������������� 3\\nDomains of Generative AI  �������������������������������������������������������������������������������������������������������������� 4\\nText Generation  ������������������������������������������������������������������������������������������������������������������������ 4\\nImage Generation  �������������������������������������������������������������������������������������������������������������������� 4\\nAudio Generation  ��������������������������������������������������������������������������������������������������������������������� 5\\nVideo Generation  �������������������������������������������������������������������������', '�������������������������������������������� 5\\nGenerative AI: Current Players and Their Models  ��������������������������������������������������������������������� 9\\nGenerative AI Applications  ����������������������������������������������������������������������������������������������������� 11\\nConclusion  ���������������������������������������������������������������������������������������������������������������������������������� 13\\nChapter 2:   Evolution of Neural Networks to Large Language Models  �������������������� 15\\nNatural Language Processing  ����������������������������������������������������������������������������������������������������� 16\\nTokenization  �������������������������������������������������������������������������������������������������������������������������� 17\\nN-grams  �������������������������������������������������������������������������������������������������������������������������������� 17\\nLanguage Representation and Embeddings  ������', '�������������������������������������������������������������������� 19\\nProbabilistic Models  �������������������������������������������������������������������������������������������������������������������� 20\\nNeural Network–Based Language Models  ���������������������������������������������������������������������������������� 21\\nRecurrent Neural Networks (RNNs)  ��������������������������������������������������������������������������������������� 22\\nLong Short-Term Memory (LSTM)  ����������������������������������������������������������������������������������������� 23\\nGated Recurrent Unit (GRU)  ��������������������������������������������������������������������������������������������������� 24\\nEncoder-Decoder Networks  ��������������������������������������������������������������������������������������������������� 25viTransformer  ��������������������������������������������������������������������������������������������������������������������������������� 27\\nLarge ', 'Language Models (LLMs)  �������������������������������������������������������������������������������������������������� 29\\nConclusion  ���������������������������������������������������������������������������������������������������������������������������������� 30\\nChapter 3:   LLMs and Transformers  ������������������������������������������������������������������������� 33\\nThe Power of Language Models  �������������������������������������������������������������������������������������������������� 33\\nTransformer Architecture  ������������������������������������������������������������������������������������������������������������ 34\\nMotivation for Transformer  ���������������������������������������������������������������������������������������������������� 35\\nArchitecture  ��������������������������������������������������������������������������������������������������������������������������� 35\\nEncoder-Decoder Architecture �������������������������������������������', '���������������������������������������������������� 36\\nAttention  �������������������������������������������������������������������������������������������������������������������������������� 39\\nPosition-wise Feed-Forward Networks  ��������������������������������������������������������������������������������� 47\\nAdvantages and Limitations of Transformer Architecture  ������������������������������������������������������ 51\\nConclusion  ���������������������������������������������������������������������������������������������������������������������������������� 53\\nChapter 4:   The ChatGPT Architecture: An In-Depth Exploration of OpenAI’s \\nConversational Language Model  ����������������������������������������������������������� 55\\nThe Evolution of GPT Models ������������������������������������������������������������������������������������������������������� 56\\nThe Transformer Architecture: A Recap  ���������������������������������������������������������������������������', '������������ 57\\nArchitecture of ChatGPT  �������������������������������������������������������������������������������������������������������������� 59\\nPre-training and Fine-Tuning in ChatGPT  ������������������������������������������������������������������������������������ 70\\nPre-training: Learning Language Patterns  ����������������������������������������������������������������������������� 70\\nFine-Tuning: Adapting to Specific Tasks  �������������������������������������������������������������������������������� 71\\nContinuous Learning and Iterative Improvement  ������������������������������������������������������������������ 71\\nContextual Embeddings in ChatGPT  �������������������������������������������������������������������������������������������� 71\\nResponse Generation in ChatGPT  ������������������������������������������������������������������������������������������������ 72\\nHandling Biases and Ethical Considerations  ���������������������������������������������', '���������������������������������� 73\\nAddressing Biases in Language Models  �������������������������������������������������������������������������������� 73\\nOpenAI’s Efforts to Mitigate Biases  ��������������������������������������������������������������������������������������� 73\\nStrengths and Limitations  ����������������������������������������������������������������������������������������������������������� 75\\nStrengths of ChatGPT  ������������������������������������������������������������������������������������������������������������ 75\\nLimitations of ChatGPT  ���������������������������������������������������������������������������������������������������������� 76\\nConclusion  ���������������������������������������������������������������������������������������������������������������������������������� 77Table of Con TenTsviiChapter 5:   Google Bard and Beyond  ����������������������������������������������������������������������� 79\\nThe Transformer Architecture', '  ����������������������������������������������������������������������������������������������������� 80\\nElevating Transformer: The Genius of Google Bard  ��������������������������������������������������������������������� 80\\nGoogle Bard’s Text and Code Fusion  �������������������������������������������������������������������������������������� 82\\nStrengths and Weaknesses of Google Bard  �������������������������������������������������������������������������������� 83\\nStrengths  ������������������������������������������������������������������������������������������������������������������������������� 83\\nWeaknesses  �������������������������������������������������������������������������������������������������������������������������� 84\\nDifference Between ChatGPT and Google Bard  ��������������������������������������������������������������������������� 84\\nClaude 2  ����������������������������������������������������������������������������������������������������������', '���������������������������� 86\\nKey Features of Claude 2  ������������������������������������������������������������������������������������������������������� 86\\nComparing Claude 2 to Other AI Chatbots  ����������������������������������������������������������������������������� 87\\nThe Human-Centered Design Philosophy of Claude  �������������������������������������������������������������� 88\\nExploring Claude’s AI Conversation Proficiencies  ������������������������������������������������������������������ 89\\nConstitutional AI ��������������������������������������������������������������������������������������������������������������������� 89\\nClaude 2 vs � GPT 3 �5 �������������������������������������������������������������������������������������������������������������� 92\\nOther Large Language Models  ���������������������������������������������������������������������������������������������������� 93\\nFalcon AI  �����������������������������������������������������������', '��������������������������������������������������������������������� 93\\nLLaMa 2  ��������������������������������������������������������������������������������������������������������������������������������� 95\\nDolly 2  ������������������������������������������������������������������������������������������������������������������������������������ 98\\nConclusion  ���������������������������������������������������������������������������������������������������������������������������������� 99\\nChapter 6:   Implement LLMs Using Sklearn  ���������������������������������������������������������� 101\\nInstall Scikit-LLM and Setup  ����������������������������������������������������������������������������������������������������� 102\\nObtain an OpenAI API Key  ���������������������������������������������������������������������������������������������������� 103\\nZero-Shot GPTClassifier  ������������������������������������������������������������������������������������������������', '������������ 103\\nWhat If You Find Yourself Without Labeled Data?  ���������������������������������������������������������������� 109\\nMultilabel Zero-Shot Text Classification  ������������������������������������������������������������������������������������ 111\\nImplementation  ������������������������������������������������������������������������������������������������������������������� 111\\nWhat If You Find Yourself Without Labeled Data?  ���������������������������������������������������������������� 112\\nImplementation  ������������������������������������������������������������������������������������������������������������������� 112Table of Con TenTsviiiText Vectorization  ���������������������������������������������������������������������������������������������������������������������� 113\\nImplementation  ������������������������������������������������������������������������������������������������������������������� 113\\nText Summarization  ������������������������', '������������������������������������������������������������������������������������������ 114\\nImplementation  ������������������������������������������������������������������������������������������������������������������� 115\\nConclusion  �������������������������������������������������������������������������������������������������������������������������������� 115\\nChapter 7:   LLMs for Enterprise and LLMOps  �������������������������������������������������������� 117\\nPrivate Generalized LLM API  ����������������������������������������������������������������������������������������������������� 118\\nDesign Strategy to Enable LLMs for Enterprise: In-Context Learning  ��������������������������������������� 119\\nData Preprocessing/Embedding  ������������������������������������������������������������������������������������������ 121\\nPrompt Construction/Retrieval  �������������������������������������������������������������������������������������������� 123\\nFine-Tuning  �����', '�������������������������������������������������������������������������������������������������������������������������� 126\\nTechnology Stack  ���������������������������������������������������������������������������������������������������������������������� 128\\nGen AI/LLM Testbed  ������������������������������������������������������������������������������������������������������������� 128\\nData Sources  ����������������������������������������������������������������������������������������������������������������������� 129\\nData Processing ������������������������������������������������������������������������������������������������������������������� 129\\nLeveraging Embeddings for Enterprise LLMs  ���������������������������������������������������������������������� 130\\nVector Databases: Accelerating Enterprise LLMs with Semantic Search  ���������������������������� 130\\nLLM APIs: Empowering Enterprise Language Capabilities  �������������������������������������������������', '� 130\\nLLMOps  ������������������������������������������������������������������������������������������������������������������������������������� 131\\nWhat Is LLMOps?  ����������������������������������������������������������������������������������������������������������������� 131\\nWhy LLMOps?  ���������������������������������������������������������������������������������������������������������������������� 133\\nWhat Is an LLMOps Platform?  ��������������������������������������������������������������������������������������������� 134\\nTechnology Components LLMOps  ���������������������������������������������������������������������������������������� 135\\nMonitoring Generative AI Models  ����������������������������������������������������������������������������������������� 136\\nProprietary Generative AI Models  ���������������������������������������������������������������������������������������� 139\\nOpen Source Models with Permissive Licenses  ����������������������������', '�������������������������������������� 140\\nPlayground for Model Selection  ������������������������������������������������������������������������������������������ 141\\nEvaluation Metrics  ��������������������������������������������������������������������������������������������������������������� 141\\nValidating LLM Outputs  �������������������������������������������������������������������������������������������������������� 144\\nChallenges Faced When Deploying LLMs  ���������������������������������������������������������������������������� 146Table of Con TenTsixImplementation  ������������������������������������������������������������������������������������������������������������������������� 148\\nUsing the OpenAI API with Python  ��������������������������������������������������������������������������������������� 148\\nLeveraging Azure OpenAI Service  ���������������������������������������������������������������������������������������� 153\\nConclusion  �����������������', '��������������������������������������������������������������������������������������������������������������� 153\\nChapter 8:   Diffusion Model and Generative AI for Images  ������������������������������������ 155\\nVariational Autoencoders (VAEs)  ����������������������������������������������������������������������������������������������� 156\\nGenerative Adversarial Networks (GANs)  ���������������������������������������������������������������������������������� 157\\nDiffusion Models  ����������������������������������������������������������������������������������������������������������������������� 158\\nTypes of Diffusion Models  ��������������������������������������������������������������������������������������������������� 160\\nArchitecture  ������������������������������������������������������������������������������������������������������������������������� 162\\nThe Technology Behind DALL-E 2 ��������������������������������������������������������������������������������', '�������������� 165\\nTop Part: CLIP Training Process  ������������������������������������������������������������������������������������������� 167\\nBottom Part: Text-to-Image Generation Process  ����������������������������������������������������������������� 168\\nThe Technology Behind Stable Diffusion  ����������������������������������������������������������������������������������� 168\\nLatent Diffusion Model (LDM)  ���������������������������������������������������������������������������������������������� 169\\nBenefits and Significance  ���������������������������������������������������������������������������������������������������� 170\\nThe Technology Behind Midjourney  ������������������������������������������������������������������������������������������ 170\\nGenerative Adversarial Networks (GANs) ����������������������������������������������������������������������������� 170\\nText-to-Image Synthesis with GANs  �������������������������������������������������������������', '����������������������� 171\\nConditional GANs  ����������������������������������������������������������������������������������������������������������������� 171\\nTraining Process  ������������������������������������������������������������������������������������������������������������������ 171\\nLoss Functions and Optimization  ����������������������������������������������������������������������������������������� 171\\nAttention Mechanisms  ��������������������������������������������������������������������������������������������������������� 172\\nData Augmentation and Preprocessing  ������������������������������������������������������������������������������� 172\\nBenefits and Applications  ���������������������������������������������������������������������������������������������������� 172\\nComparison Between DALL-E 2, Stable Diffusion, and Midjourney  ������������������������������������������ 172\\nApplications  ���������������������������������������������������������������', '��������������������������������������������������������������� 174\\nConclusion  �������������������������������������������������������������������������������������������������������������������������������� 176Table of Con TenTsxChapter 9:   ChatGPT Use Cases  ������������������������������������������������������������������������������ 179\\nBusiness and Customer Service  ����������������������������������������������������������������������������������������������� 179\\nContent Creation and Marketing  ����������������������������������������������������������������������������������������������� 181\\nSoftware Development and Tech Support  ��������������������������������������������������������������������������������� 183\\nData Entry and Analysis  ������������������������������������������������������������������������������������������������������������ 185\\nHealthcare and Medical Information  ����������������������������������������������������������������������������������������� 18', '7\\nMarket Research and Analysis  �������������������������������������������������������������������������������������������������� 189\\nCreative Writing and Storytelling  ���������������������������������������������������������������������������������������������� 191\\nEducation and Learning  ������������������������������������������������������������������������������������������������������������ 193\\nLegal and Compliance  ��������������������������������������������������������������������������������������������������������������� 194\\nHR and Recruitment  ������������������������������������������������������������������������������������������������������������������ 196\\nPersonal Assistant and Productivity  ������������������������������������������������������������������������������������������ 198\\nExamples ����������������������������������������������������������������������������������������������������������������������������������� 200\\nConclusion  ����������������������������', '���������������������������������������������������������������������������������������������������� 205\\n Index  ��������������������������������������������������������������������������������������������������������������������� 207Table of Con TenTsxiAkshay\\xa0 Kulkarni \\xa0is an AI and machine learning evangelist \\nand IT leader. He has assisted numerous Fortune 500 and \\nglobal firms in advancing strategic transformations using \\nAI and data science. He is a Google Developer Expert, \\nauthor, and regular speaker at major AI and data science \\nconferences (including Strata, O’Reilly AI Conf, and GIDS). \\nHe is also a visiting faculty member for some of the top \\ngraduate institutes in India. In 2019, he was featured as one \\nof the top 40 under-40 data scientists in India. He enjoys \\nreading, writing, coding, and building next-gen AI products.   \\nAdarsha\\xa0 Shivananda \\xa0is a data science and generative AI \\nleader. Presently, he is focused on creating world-  class \\nMLOps and LLMOps capabilities to ', 'ensure continuous value \\ndelivery using AI.\\xa0He aims to build a pool of exceptional \\ndata scientists within and outside the organization to solve \\nproblems through training programs and always wants \\nto stay ahead of the curve. He has worked in the pharma, \\nhealthcare, CPG, retail, and marketing industries. He lives in \\nBangalore and loves to read and teach data science.   \\nAnoosh\\xa0 Kulkarni \\xa0is a data scientist and MLOps engineer. He \\nhas worked with various global enterprises across multiple \\ndomains solving their business problems using machine \\nlearning and AI.\\xa0He has worked at one of the leading \\necommerce giants in UAE, where he focused on building \\nstate-of-the-art recommender systems and deep learning–\\nbased search engines. He is passionate about guiding and \\nmentoring people in their data science journey. He often \\nleads data science/machine learning meetups, helping aspiring data scientists carve \\ntheir career road map.   About the Authors\\nxiiDilip\\xa0 Gudivada  is a seasoned seni', \"or data architect with \\n13 years of experience in cloud services, big data, and data \\nengineering. Dilip has a strong background in designing and \\ndeveloping ETL solutions, focusing specifically on building \\nrobust data lakes on the Azure cloud platform. Leveraging \\ntechnologies such as Azure Databricks, Data Factory, Data \\nLake Storage, PySpark, Synapse, and Log Analytics, Dilip \\nhas helped organizations establish scalable and efficient \\ndata lake solutions on Azure. He has a deep understanding \\nof cloud services and a track record of delivering successful \\ndata engineering projects.   \\nabouT The auThorsxiiiPrajwal  is a lead applied scientist and consultant in the \\nfield of generative AI.\\xa0He is passionate about building AI \\napplications in the service of humanity.  About the Technical Reviewer\\nxvIntroduction\\nWelcome to Applied Generative AI for Beginners: Practical Knowledge on Diffusion \\nModels, ChatGPT, and Other LLMs . Within these pages, you're about to embark on an \\nexhilarating\", \" journey into the world of generative artificial intelligence (AI). This book \\nserves as a comprehensive guide that not only unveils the intricacies of generative AI but \\nalso equips you with the knowledge and skills to implement it.\\nIn recent years, generative AI has emerged as a powerhouse of innovation, reshaping \\nthe technological landscape and redefining the boundaries of what machines can \\nachieve. At its core, generative AI empowers artificial systems to understand and \\ngenerate human language with remarkable fluency and creativity. As we delve deep \\ninto this captivating landscape, you'll gain both a theoretical foundation and practical \\ninsights into this cutting-edge field.\\n What You\\xa0Will Discover\\nThroughout the chapters of this book, you will\\n• Build Strong Foundations: Develop a solid understanding of the core \\nprinciples that drive generative AI's capabilities, enabling you to \\ngrasp its inner workings.\\n• Explore Cutting-Edge Architectures: Examine the architecture of \\nlar\", 'ge language models (LLMs) and transformers, including renowned \\nmodels like ChatGPT and Google Bard, to understand how these \\nmodels have revolutionized AI.\\n• Master Practical Implementations: Acquire hands-on skills for \\nintegrating generative AI into your projects, with a focus on \\nenterprise-grade solutions and fine-tuning techniques that enable \\nyou to tailor AI to your specific needs.xvi• Operate with Excellence: Discover LLMOps, the operational \\nbackbone of managing generative AI models, ensuring efficiency, \\nreliability, and security in your AI deployments.\\n• Witness Real-World Use Cases: Explore how generative AI is \\nrevolutionizing diverse domains, from business and healthcare to \\ncreative writing and legal compliance, through a rich tapestry of real-  \\nworld use cases.InTrodu CTIon1\\n© Akshay Kulkarni, Adarsha Shivananda, Anoosh Kulkarni, Dilip Gudivada 2023 \\nA. Kulkarni et al., Applied Generative AI for Beginners , https://doi.org/10.1007/978-1-4842-9994-4_1CHAPTER 1\\nIntroduc', 'tion to\\xa0Generative AI\\nHave you ever imagined that simply by picturing something and typing, an image or \\nvideo could be generated? How fascinating is that? This concept, once relegated to \\nthe realm of science fiction, has become a tangible reality in our modern world. The \\nidea that our thoughts and words can be transformed into visual content is not only \\ncaptivating but a testament to human innovation and creativity.\\nEven as data scientists, many of us never anticipated that AI could reach a point \\nwhere it could generate text for a specific use case. The struggles we faced in writing \\ncode or the countless hours spent searching on Google for the right solution were once \\ncommon challenges. Yet, the technological landscape has shifted dramatically, and \\nthose laborious tasks have become relics of the past.\\nFigure 1-1.  The machine-generated image based on text input2How has this become possible? The answer lies in the groundbreaking \\nadvancements in deep learning and natural languag', 'e processing (NLP). These \\ntechnological leaps have paved the way for generative AI, a field that harnesses the \\npower of algorithms to translate thoughts into visual representations or automates the \\ncreation of complex code. Thanks to these developments, we’re now experiencing a \\nfuture where imagination and innovation intertwine, transforming the once-unthinkable \\ninto everyday reality.\\n So, What Is Generative AI?\\nGenerative AI  refers to a branch of artificial intelligence that focuses on creating models \\nand algorithms capable of generating new, original content, such as images, text, music, \\nand even videos. Unlike traditional AI models that are trained to perform specific tasks, \\ngenerative AI models aim to learn and mimic patterns from existing data to generate \\nnew, unique outputs.\\nGenerative AI has a wide range of applications. For instance, in computer \\nvision, generative models can generate realistic images, create variations of existing \\nimages, or even complete missing pa', 'rts of an image. In natural language processing, \\ngenerative models can be used for language translation, text synthesis, or even to create \\nconversational agents that produce humanlike responses. Beyond these examples, \\ngenerative ai can perform art generation, data augmentation, and even generating \\nsynthetic medical images for research and diagnosis. It’s a powerful and creative tool \\nthat allows us to explore the boundaries of what’s possible in computer vision.\\nHowever, it’s worth noting that generative AI also raises ethical concerns. The ability \\nto generate realistic and convincing fake content can be misused for malicious purposes, \\nsuch as creating deepfakes or spreading disinformation. As a result, there is ongoing \\nresearch and development of techniques to detect and mitigate the potential negative \\nimpacts of generative AI.\\nOverall, generative AI holds great promise for various creative, practical applications \\nand for generating new and unique content. It continues to be ', 'an active area of research \\nand development, pushing the boundaries of what machines can create and augmenting \\nhuman creativity in new and exciting ways.Chapter 1  Introdu CtIon to\\xa0Generat Ive aI3 Components of\\xa0AI\\n• Artificial Intelligence (AI): It is the broader discipline of machine \\nlearning to perform tasks that would typically require human \\nintelligence.\\n• Machine Learning (ML): A subset of AI, ML involves algorithms \\nthat allow computers to learn from data rather than being explicitly \\nprogrammed to do so.\\n• Deep Learning (DL): A specialized subset of ML, deep learning \\ninvolves neural networks with three or more layers that can analyze \\nvarious factors of a dataset.\\n• Generative AI: An advanced subset of AI and DL, generative AI \\nfocuses on creating new and unique outputs. It goes beyond the \\nscope of simply analyzing data to making new creations based on \\nlearned patterns.\\nFigure 1-2 explains how generative AI is a component of AI.\\nFigure 1-2.  AI and its componentsChapter 1 ', ' Introdu CtIon to\\xa0Generat Ive aI4 Domains of\\xa0Generative AI\\nLet’s deep dive into domains of generative AI in detail, including what it is, how it works, \\nand some practical applications.\\n Text Generation\\n• What It Is: Text generation involves using AI models to create \\nhumanlike text based on input prompts.\\n• How It Works: Models like GPT-3 use Transformer architectures. \\nThey’re pre-trained on vast text datasets to learn grammar, context, \\nand semantics. Given a prompt, they predict the next word or phrase \\nbased on patterns they’ve learned.\\n• Applications: Text generation is applied in content creation, chatbots, \\nand code generation. Businesses can use it for crafting blog posts, \\nautomating customer support responses, and even generating code \\nsnippets. Strategic thinkers can harness it to quickly draft marketing \\ncopy or create personalized messages for customers.\\n Image Generation\\n• What It Is: Image generation involves using various deep learning \\nmodels to create images that loo', 'k real.\\n• How It Works: GANs consist of a generator (creates images) and a \\ndiscriminator (determines real vs. fake). They compete in a feedback \\nloop, with the generator getting better at producing images that the \\ndiscriminator can’t distinguish from real ones.\\n• Applications: These models are used in art, design, and product \\nvisualization. Businesses can generate product mock-ups for \\nadvertising, create unique artwork for branding, or even generate \\nfaces for diverse marketing materials.Chapter 1  Introdu CtIon to\\xa0Generat Ive aI5 Audio Generation\\n• What It Is: Audio generation involves AI creating music, sounds, or \\neven humanlike voices.\\n• How It Works: Models like WaveGAN analyze and mimic audio \\nwaveforms. Text-to-speech models like Tacotron 2 use input text \\nto generate speech. They’re trained on large datasets to capture \\nnuances of sound.\\n• Applications: AI-generated music can be used in ads, videos, or \\nas background tracks. Brands can create catchy jingles or custom \\nsound', ' effects for marketing campaigns. Text-to-speech technology \\ncan automate voiceovers for ads or customer service interactions. \\nStrategically, businesses can use AI-generated audio to enhance \\nbrand recognition and storytelling.\\n Video Generation\\n• What It Is: Video generation involves AI creating videos, often by \\ncombining existing visuals or completing missing parts.\\n• How It Works: Video generation is complex due to the temporal \\nnature of videos. Some models use text descriptions to generate \\nscenes, while others predict missing frames in videos.\\n• Applications: AI-generated videos can be used in personalized \\nmessages, dynamic ads, or even content marketing. Brands can craft \\nunique video advertisements tailored to specific customer segments. \\nThoughtful application can lead to efficient video content creation \\nthat adapts to marketing trends.\\n Generating Images\\nMicrosoft Bing Image Creator is a generative AI tool that uses artificial intelligence to \\ncreate images based on your ', 'text descriptions.\\nwww.bing.com/images/create/Chapter 1  Introdu CtIon to\\xa0Generat Ive aI6To use Bing Image Creator, you simply type a description of the image you want to \\ncreate into the text box. We will use the same example mentioned earlier in generating \\nrealistic images. “ Create an image of a pink elephant wearing a party hat and \\nstanding on a rainbow. ” Bing Image Creator will then generate an image based on your \\ndescription.\\nFigure 1-3 shows the Microsoft Bing output.\\nFigure 1-3.  Microsoft Bing output\\n Generating Text\\nLet’s use ChatGPT for generating text. It is a large language model–based chatbot \\ndeveloped by OpenAI and launched in November 2022.\\nChatGPT is trained with reinforcement learning through human feedback and \\nreward models that rank the best responses. This feedback helps augment ChatGPT with \\nmachine learning to improve future responses.\\nChatGPT can be used for a variety of purposes, including\\n• Having conversations with users\\n• Answering questions\\n• Generati', 'ng textChapter 1  Introdu CtIon to\\xa0Generat Ive aI7Figure 1-4.  ChatGPT’s output\\n• Translating languages\\n• Writing different kinds of creative content\\nChatGPT can be accessed online at\\nhttps://openai.com/blog/chatgpt\\nTo use ChatGPT, you simply type a description you want into the text box.\\nTo create content on our solar system. Figure\\xa0 1-4 shows the ChatGPT’s output.Chapter 1  Introdu CtIon to\\xa0Generat Ive aI8\\nFigure 1-4.  (continued)\\nChatGPT or any other tools are still under development, but it has learned to \\nperform many kinds of tasks. As it continues to learn, it will become even more powerful \\nand versatile.Chapter 1  Introdu CtIon to\\xa0Generat Ive aI9 Generative AI: Current Players and\\xa0Their Models\\nGenerative AI is a rapidly growing field with the potential to revolutionize many \\nindustries. Figure\\xa0 1-5 shows some of the current players in the generative AI space.\\nFigure 1-5.  ChatGPT’s output\\nBriefly let’s discuss few of them:\\n• OpenAI: OpenAI is a generative AI research company t', 'hat was \\nfounded by Elon Musk, Sam Altman, and others. OpenAI has \\ndeveloped some of the most advanced generative AI models in the \\nworld, including GPT-4 and DALL-E 2.\\n• GPT-4: GPT-4 is a large language model that can generate text, \\ntranslate languages, write different kinds of creative content, and \\nanswer your questions in an informative way.\\n• DALL-E 2: DALL-E 2 is a generative AI model that can create \\nrealistic images from text descriptions.\\n• DeepMind: DeepMind is a British artificial intelligence company that \\nwas acquired by Google in 2014. DeepMind has developed several \\ngenerative AI models, including AlphaFold, which can predict the \\nstructure of proteins, and Gato, which can perform a variety of tasks, \\nincluding playing Atari games, controlling robotic arms, and writing \\ndifferent kinds of creative content.Chapter 1  Introdu CtIon to\\xa0Generat Ive aI10• Anthropic: Anthropic is a company that is developing generative \\nAI models for use in a variety of industries, including ', 'healthcare, \\nfinance, and manufacturing. Anthropic’s models are trained on \\nmassive datasets of real-world data, which allows them to generate \\nrealistic and accurate outputs.\\n• Synthesia: Synthesia is a company that specializes in creating realistic \\nsynthetic media, such as videos and audio recordings. Synthesia’s \\ntechnology can be used to create avatars that can speak, gesture, and \\neven lip-sync to any audio input.\\n• RealSpeaker: RealSpeaker is a generative AI model that can be \\nused to create realistic synthetic voices.\\n• Natural Video: Natural Video is a generative AI model that can be \\nused to create realistic synthetic videos.\\n• RunwayML: RunwayML is a platform that makes it easy for \\nbusinesses to build and deploy generative AI models. RunwayML \\nprovides a variety of tools and resources to help businesses collect \\ndata, train models, and evaluate results.\\n• Runway Studio: Runway Studio is a cloud-based platform that \\nallows businesses to build and deploy generative AI models ']\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'raw_items'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[40], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m chunks \u001b[38;5;241m=\u001b[39m split_text(text)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Generate embeddings\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m chunk_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(chunk_embeddings)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Add embeddings to FAISS index\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[36], line 12\u001b[0m, in \u001b[0;36mcreate_embeddings\u001b[1;34m(texts, batch_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m     batch \u001b[38;5;241m=\u001b[39m texts[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[1;32m---> 12\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membed-english-v3.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearch_document\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(response\u001b[38;5;241m.\u001b[39membeddings)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mvstack(embeddings)\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\cohere-5.9.2-py3.12.egg\\cohere\\client.py:206\u001b[0m, in \u001b[0;36mClient.embed\u001b[1;34m(self, texts, images, model, input_type, embedding_types, truncate, request_options, batching)\u001b[0m\n\u001b[0;32m    203\u001b[0m textsarr: typing\u001b[38;5;241m.\u001b[39mSequence[\u001b[38;5;28mstr\u001b[39m]  \u001b[38;5;241m=\u001b[39m texts \u001b[38;5;28;01mif\u001b[39;00m texts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OMIT \u001b[38;5;129;01mand\u001b[39;00m texts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    204\u001b[0m texts_batches \u001b[38;5;241m=\u001b[39m [textsarr[i : i \u001b[38;5;241m+\u001b[39m embed_batch_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(textsarr), embed_batch_size)]\n\u001b[1;32m--> 206\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext_batch\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mBaseCohere\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43membedding_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merge_embed_responses(responses)\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\cohere-5.9.2-py3.12.egg\\cohere\\client.py:209\u001b[0m, in \u001b[0;36mClient.embed.<locals>.<lambda>\u001b[1;34m(text_batch)\u001b[0m\n\u001b[0;32m    203\u001b[0m textsarr: typing\u001b[38;5;241m.\u001b[39mSequence[\u001b[38;5;28mstr\u001b[39m]  \u001b[38;5;241m=\u001b[39m texts \u001b[38;5;28;01mif\u001b[39;00m texts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OMIT \u001b[38;5;129;01mand\u001b[39;00m texts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    204\u001b[0m texts_batches \u001b[38;5;241m=\u001b[39m [textsarr[i : i \u001b[38;5;241m+\u001b[39m embed_batch_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(textsarr), embed_batch_size)]\n\u001b[0;32m    206\u001b[0m responses \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    207\u001b[0m     response\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m--> 209\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m text_batch: \u001b[43mBaseCohere\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43membedding_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    218\u001b[0m         texts_batches,\n\u001b[0;32m    219\u001b[0m     )\n\u001b[0;32m    220\u001b[0m ]\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merge_embed_responses(responses)\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\cohere-5.9.2-py3.12.egg\\cohere\\base_client.py:1848\u001b[0m, in \u001b[0;36mBaseCohere.embed\u001b[1;34m(self, texts, images, model, input_type, embedding_types, truncate, request_options)\u001b[0m\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\n\u001b[0;32m   1769\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1770\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1777\u001b[0m     request_options: typing\u001b[38;5;241m.\u001b[39mOptional[RequestOptions] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1778\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EmbedResponse:\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;124;03m    This endpoint returns text embeddings. An embedding is a list of floating point numbers that captures semantic information about the text that it represents.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;124;03m    client.embed()\u001b[39;00m\n\u001b[0;32m   1847\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1848\u001b[0m     _response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttpx_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1849\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mv1/embed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m   1852\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtexts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1853\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1855\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1856\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1857\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtruncate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1858\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1859\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43momit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOMIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1863\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m _response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\cohere-5.9.2-py3.12.egg\\cohere\\core\\http_client.py:195\u001b[0m, in \u001b[0;36mHttpClient.request\u001b[1;34m(self, path, method, base_url, params, json, data, content, files, headers, request_options, retries, omit)\u001b[0m\n\u001b[0;32m    187\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    188\u001b[0m     request_options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout_in_seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m request_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m request_options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout_in_seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_timeout\n\u001b[0;32m    191\u001b[0m )\n\u001b[0;32m    193\u001b[0m json_body, data_body \u001b[38;5;241m=\u001b[39m get_request_body(json\u001b[38;5;241m=\u001b[39mjson, data\u001b[38;5;241m=\u001b[39mdata, request_options\u001b[38;5;241m=\u001b[39mrequest_options, omit\u001b[38;5;241m=\u001b[39momit)\n\u001b[1;32m--> 195\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttpx_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murljoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjsonable_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_none_from_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madditional_headers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsonable_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_none_from_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m                \u001b[49m\u001b[43mremove_omit_from_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madditional_query_parameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    216\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m                    \u001b[49m\u001b[43momit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_file_dict_to_httpx_tuples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_none_from_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m max_retries: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m request_options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_retries\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m request_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_retry(response\u001b[38;5;241m=\u001b[39mresponse):\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py:815\u001b[0m, in \u001b[0;36mClient.request\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m    800\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m    802\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[0;32m    803\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    804\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    813\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    814\u001b[0m )\n\u001b[1;32m--> 815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py:902\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    894\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[0;32m    898\u001b[0m )\n\u001b[0;32m    900\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 902\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py:930\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    927\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 930\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py:967\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    965\u001b[0m     hook(request)\n\u001b[1;32m--> 967\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py:1003\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    999\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1000\u001b[0m     )\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1003\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1007\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_transports\\default.py:218\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    205\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    206\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    207\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    216\u001b[0m )\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 218\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    223\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    224\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    225\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    226\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    227\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:253\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_closed(status)\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:237\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool_lock:\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;66;03m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;66;03m# status so that the request becomes queued again.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\connection.py:90\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\http11.py:105\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp11.response_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\http11.py:84\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request_body(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp11.receive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, request, kwargs\n\u001b[0;32m     78\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m     79\u001b[0m     (\n\u001b[0;32m     80\u001b[0m         http_version,\n\u001b[0;32m     81\u001b[0m         status,\n\u001b[0;32m     82\u001b[0m         reason_phrase,\n\u001b[0;32m     83\u001b[0m         headers,\n\u001b[1;32m---> 84\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     86\u001b[0m         http_version,\n\u001b[0;32m     87\u001b[0m         status,\n\u001b[0;32m     88\u001b[0m         reason_phrase,\n\u001b[0;32m     89\u001b[0m         headers,\n\u001b[0;32m     90\u001b[0m     )\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m     93\u001b[0m     status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[0;32m     94\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m     },\n\u001b[0;32m    101\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\cyril\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\http11.py:156\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    152\u001b[0m http_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHTTP/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m event\u001b[38;5;241m.\u001b[39mhttp_version\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# h11 version 0.11+ supports a `raw_items` interface to get the\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# raw header casing, rather than the enforced lowercase headers.\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_items\u001b[49m()\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m http_version, event\u001b[38;5;241m.\u001b[39mstatus_code, event\u001b[38;5;241m.\u001b[39mreason, headers\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'raw_items'"
          ]
        }
      ],
      "source": [
        "pdf_path = r\"C:\\Users\\cyril\\Downloads\\Applied-Generative-AI-for-Beginners.pdf\"\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "chunks = split_text(text)\n",
        "\n",
        "# Generate embeddings\n",
        "chunk_embeddings = create_embeddings(chunks)\n",
        "print(chunk_embeddings)\n",
        "# Add embeddings to FAISS index\n",
        "index.add(np.array(chunk_embeddings).astype(np.float32))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mw3xG69Ltlq"
      },
      "source": [
        "## Function to retrive relevent information from the documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "d44tq_me9lpR"
      },
      "outputs": [],
      "source": [
        "def retrieve(query, index, k=3):\n",
        "\n",
        "    query_embed = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_document\").embeddings\n",
        "\n",
        "    D, I = index.search(np.array(query_embed).astype(np.float32), k)\n",
        "\n",
        "    return [chunks[i] for i in I[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsuc1MC1MbpB"
      },
      "source": [
        "## Generation of the text from prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "uBDw4ND-9n5Z"
      },
      "outputs": [],
      "source": [
        "query = \"What is Generative AI and its applications?\"\n",
        "context = retrieve(query, index)\n",
        "\n",
        "contexts = \"\"\n",
        "for cont in context:\n",
        "  contexts = contexts + cont\n",
        "\n",
        "prompt = f\"**Context/Knowledge**: {contexts} \\n\\n **Query**: {query}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSzoA-tdMmSw"
      },
      "source": [
        "## Streaming the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "annIc2CgB1od",
        "outputId": "9cdd2134-2787-49ee-dcd7-e29ed175e4b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Generative AI:\n",
            "Generative AI is a fascinating and rapidly evolving field within artificial intelligence. It involves the development of advanced algorithms and models that can create new and diverse content, mimicking the creative process of humans. Unlike traditional AI systems that are designed for specific tasks, generative AI focuses on learning patterns and structures from existing data to produce novel outputs.\n",
            "\n",
            "## Applications of Generative AI:\n",
            "1. **Text Generation**:\n",
            "   - Generative AI models can write creative stories, news articles, poetry, and even code. For example, GPT (Generative Pre-trained Transformer) models have gained significant attention for their ability to generate coherent and contextually relevant text.\n",
            "   - These models can assist content creators, writers, and marketers in generating ideas, outlines, and drafts, thereby increasing productivity.\n",
            "\n",
            "2. **Image Generation**:\n",
            "   - Generative AI can create realistic images, artwork, and even modify or enhance existing visuals. Models like Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) are widely used for this purpose.\n",
            "   - Applications include generating product images for e-commerce, creating virtual avatars, and enhancing image quality in photography and graphic design.\n",
            "\n",
            "3. **Audio Generation**:\n",
            "   - This technology can synthesize speech, music, and sound effects. WaveNet, a deep generative model for raw audio waveforms, has been used to generate human-like speech.\n",
            "   - Applications range from creating personalized voice assistants to composing music and generating sound effects for games and movies.\n",
            "\n",
            "4. **Video Generation**:\n",
            "   - Generative AI models can create videos, animations, and simulations. This includes generating realistic human actions, synthesizing video content, and even creating virtual environments.\n",
            "   - It has applications in the film and gaming industries, virtual reality, and content creation for various media platforms.\n",
            "\n",
            "5. **Data Augmentation**:\n",
            "   - Generative models can be used to create synthetic data for training other AI systems, especially in cases where real-world data is limited.\n",
            "   - This is particularly useful in fields like medical imaging, where generating diverse and realistic medical data can improve diagnostic models.\n",
            "\n",
            "6. **Creative Content Generation**:\n",
            "   - Generative AI is revolutionizing the creative industries. It can assist in generating artwork, designing fashion collections, creating marketing content, and even composing music for various genres.\n",
            "   - These models can inspire and collaborate with human artists, offering new avenues for creative expression.\n",
            "\n",
            "7. **Personalization and Recommendations**:\n",
            "   - Generative AI can be used to provide personalized recommendations in e-commerce, entertainment, and content platforms. By understanding user preferences, it can generate tailored suggestions.\n",
            "\n",
            "8. **Language Translation and Localization**:\n",
            "   - Generative models can assist in machine translation tasks, generating translations that sound more natural and fluent.\n",
            "   - This is valuable for businesses aiming to localize their content for global audiences.\n",
            "\n",
            "The applications of Generative AI are vast and continue to expand across various industries. As the technology advances, we can expect even more innovative use cases, blurring the lines between human creativity and artificial intelligence."
          ]
        }
      ],
      "source": [
        "import cohere\n",
        "\n",
        "stream = co.chat_stream(\n",
        "  model='command-r-plus-08-2024',\n",
        "  message=prompt,\n",
        "  temperature=0.4,\n",
        "  chat_history=[],\n",
        "  prompt_truncation='AUTO',\n",
        "  #connectors=[{\"id\":\"web-search\"}],\n",
        "  max_tokens=4096\n",
        ")\n",
        "\n",
        "for event in stream:\n",
        "  if event.event_type == \"text-generation\":\n",
        "    print(event.text, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK1UH5GPQcor"
      },
      "source": [
        "# **Provide several example queries**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAZ_aOQNQajU",
        "outputId": "8ef9113e-a711-4aa1-bce6-70cda902939f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diffusion models offer a unique and innovative approach to image generation by employing a process that can be likened to a series of steps, each adding a layer of complexity to the image generation process. Here's a breakdown of how diffusion models generate images:\n",
            "\n",
            "**1. Noise Schedule and Markov Chain:** The process begins with the definition of a noise schedule, which is a sequence of noise levels ranging from minimal to significant. This schedule is crucial as it determines the progression of noise introduction. The model then employs a Markov chain, a sequential process where each step corresponds to a noise level in the schedule. \n",
            "\n",
            "**2. Adding Noise and Latent Representation:** At each step of the Markov chain, the model introduces noise to the image. This is a controlled process, where the amount of noise added is determined by the diffusion rate. Simultaneously, the model also uses a latent representation model, typically a neural network, to encode the image into a latent representation. This latent representation captures the essential features and structure of the image.\n",
            "\n",
            "**3. Iterative Process and Parameter Optimization:** The diffusion process is iterative, meaning it repeats the steps of adding noise and updating the latent representation. During this process, the model aims to find the optimal parameters for the latent representation model. These parameters are adjusted to maximize the likelihood that the model could have generated the real images in the dataset. In other words, the model learns to reverse the noise addition process, gradually refining the image quality.\n",
            "\n",
            "**4. Diffusion Process and Image Generation:** The diffusion process is a probabilistic, state-transition process that generates new images. It starts with a latent representation and, over multiple steps, modifies it by adding and refining noise. The diffusion rate controls the amount of noise added at each step. As the process progresses, the model generates images that become increasingly detailed and realistic, aiming to match the quality of real images.\n",
            "\n",
            "**5. Applications:** Diffusion models have various applications in image generation, including:\n",
            "- **Image Synthesis from Text:** These models can generate images based on textual descriptions, making them useful for text-to-image applications.\n",
            "- **Style Transfer:** They can transfer the style of one image to another, allowing for creative image manipulations.\n",
            "- **Super-resolution:** Diffusion models can enhance the resolution of low-resolution images, improving their visual quality.\n",
            "\n",
            "The power of diffusion models lies in their ability to start with random noise and gradually transform it into a structured, meaningful image. This process is guided by the noise schedule, the latent representation model, and the iterative optimization of parameters. As a result, diffusion models can generate diverse and high-quality images, making them a promising tool in the field of generative AI for images."
          ]
        }
      ],
      "source": [
        "query = \"How do diffusion models work in generating images?\"\n",
        "context = retrieve(query, index)\n",
        "\n",
        "contexts = \"\"\n",
        "for cont in context:\n",
        "  contexts = contexts + cont\n",
        "prompt = f\"**Context/Knowledge**: {contexts} \\n\\n **Query**: {query}\"\n",
        "\n",
        "import cohere\n",
        "\n",
        "stream = co.chat_stream(\n",
        "  model='command-r-plus-08-2024',\n",
        "  message=prompt,\n",
        "  temperature=0.4,\n",
        "  chat_history=[],\n",
        "  prompt_truncation='AUTO',\n",
        "  #connectors=[{\"id\":\"web-search\"}],\n",
        "  max_tokens=4096\n",
        ")\n",
        "\n",
        "for event in stream:\n",
        "  if event.event_type == \"text-generation\":\n",
        "    print(event.text, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2XUvvrhQ6kF",
        "outputId": "91369223-986b-417f-e6f2-2fbd8417b6db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The architecture of ChatGPT is based on the Transformer model, a powerful neural network architecture initially introduced by Vaswani et al. in 2017. Specifically, ChatGPT utilizes a \"decoder-only\" version of the Transformer, which is well-suited for language generation tasks. The Transformer architecture consists of an encoder and a decoder, but in the case of ChatGPT, only the decoder component is used.\n",
            "\n",
            "**Architecture Components**:\n",
            "- **Decoder-Only Transformer**: The decoder in the Transformer architecture is responsible for generating output sequences. It takes an input and generates a corresponding response. In ChatGPT, the decoder is trained to produce coherent and contextually appropriate responses to user queries.\n",
            "- **Attention Mechanism**: ChatGPT employs the self-attention mechanism, a key feature of the Transformer architecture. This mechanism allows the model to weigh the importance of different parts of the input sequence when generating a response. It enables the model to capture long-range dependencies and generate contextually relevant outputs.\n",
            "- **Reinforcement Learning from Human Feedback (RLHF)**: This is a significant component of ChatGPT's architecture and training process. RLHF is a technique where the model is fine-tuned using human feedback to improve its responses. Human evaluators provide feedback on the model's responses, and this feedback is used to guide the model's learning process, encouraging more human-like and helpful interactions.\n",
            "\n",
            "**Fine-Tuning Process**:\n",
            "Fine-tuning is a crucial step in adapting the pre-trained ChatGPT model to specific tasks and domains, ensuring it provides relevant and accurate responses. Here's an overview of the fine-tuning process:\n",
            "1. **Pre-training**: Before fine-tuning, ChatGPT undergoes a pre-training phase where it learns general language patterns and representations from a vast amount of text data. This pre-training is typically done using unsupervised learning on a large corpus of text.\n",
            "2. **Domain Adaptation**: During fine-tuning, ChatGPT is exposed to domain-specific datasets relevant to the intended use case. For example, if ChatGPT is to be used for customer support, it will be fine-tuned on customer service conversations and queries. This step helps the model adapt its knowledge and language understanding to the specific domain.\n",
            "3. **User Interaction Guidance**: Fine-tuning also involves training the model to interact with users appropriately. This is achieved through reinforcement learning from human feedback (RLHF). Human evaluators provide feedback on the model's responses, rewarding desired behaviors and penalizing inappropriate or harmful responses. This process helps ChatGPT learn to generate contextually relevant, helpful, and safe outputs.\n",
            "4. **Iterative Improvement**: Fine-tuning is an iterative process. The model's performance is continuously evaluated, and adjustments are made to improve its responses. This may involve further exposure to domain-specific data, refining the RLHF process, or optimizing hyperparameters.\n",
            "5. **Contextual Embeddings**: ChatGPT uses contextual embeddings to represent user inputs and generate responses. These embeddings capture the semantic meaning of words in context, allowing the model to understand and generate coherent text.\n",
            "6. **Response Generation**: Fine-tuning teaches ChatGPT to generate responses by predicting the next word or token in a sequence, given the previous context. It learns to do this in a way that is consistent with the desired task and user expectations.\n",
            "\n",
            "The fine-tuning process allows ChatGPT to adapt to various applications, such as customer support, language translation, content generation, and more. By combining pre-training with domain-specific fine-tuning and user interaction guidance, ChatGPT can provide contextually relevant and helpful responses in interactive conversations."
          ]
        }
      ],
      "source": [
        "query = \"What is the architecture of ChatGPT and how is it fine-tuned?\"\n",
        "context = retrieve(query, index)\n",
        "\n",
        "contexts = \"\"\n",
        "for cont in context:\n",
        "  contexts = contexts + cont\n",
        "prompt = f\"**Context/Knowledge**: {contexts} \\n\\n **Query**: {query}\"\n",
        "\n",
        "import cohere\n",
        "\n",
        "stream = co.chat_stream(\n",
        "  model='command-r-plus-08-2024',\n",
        "  message=prompt,\n",
        "  temperature=0.4,\n",
        "  chat_history=[],\n",
        "  prompt_truncation='AUTO',\n",
        "  #connectors=[{\"id\":\"web-search\"}],\n",
        "  max_tokens=4096\n",
        ")\n",
        "\n",
        "for event in stream:\n",
        "  if event.event_type == \"text-generation\":\n",
        "    print(event.text, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1WW_44cRAoM",
        "outputId": "fef1c040-4e09-41b7-843e-1fabe756af2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The key differences between Google Bard and ChatGPT can be summarized as follows:\n",
            "\n",
            "**Architecture**: The most significant distinction lies in their architectural design. ChatGPT employs a decoder-only architecture, which means it is optimized for generating text. It takes input and generates a response based on that input. On the other hand, Google Bard utilizes an encoder-decoder architecture. This architecture allows Bard to both encode input and decode it to generate a response. The encoder-decoder setup enables Bard to handle tasks that require understanding and processing the input before generating an output.\n",
            "\n",
            "**Capabilities**: Both models are large language models with impressive capabilities, but they excel in different areas. ChatGPT, with its decoder-only architecture, is particularly skilled at generating text, making it excellent for tasks like language translation, summarization, and creative writing. Google Bard, however, is better at tasks that require real-world knowledge and understanding. Bard can answer questions, provide informative responses, and generate creative content by drawing upon its vast knowledge base. This makes Bard more suitable for tasks that demand factual accuracy and real-world context.\n",
            "\n",
            "**Performance**: The performance comparison between the two models may vary depending on the task. In general, ChatGPT's strength lies in its ability to generate coherent and fluent text, making it a powerful tool for language-related tasks. Google Bard, with its access to a vast amount of information, often provides more accurate and informative responses, especially for queries that require factual knowledge.\n",
            "\n",
            "**Bias**: Bias is an important consideration with any language model. The AI Ethics Team at Google AI conducted a bias comparison between ChatGPT and Bard. The results of this comparison are not explicitly mentioned in the provided context, but it is crucial to evaluate and address any potential biases in these models to ensure ethical and unbiased text generation.\n",
            "\n",
            "**Use Cases**: The choice between ChatGPT and Google Bard depends on the specific use case. For applications that require extensive text generation, creative writing, or language translation, ChatGPT might be more suitable. On the other hand, Google Bard is a better fit for tasks like answering questions, providing information, or generating content that requires real-world context and factual accuracy.\n",
            "\n",
            "The sources provided in the context offer more detailed comparisons and insights into the performance and capabilities of both models, which can be valuable for a comprehensive understanding of their differences."
          ]
        }
      ],
      "source": [
        "query = \"What are the key differences between Google Bard and ChatGPT?\"\n",
        "context = retrieve(query, index)\n",
        "\n",
        "contexts = \"\"\n",
        "for cont in context:\n",
        "  contexts = contexts + cont\n",
        "prompt = f\"**Context/Knowledge**: {contexts} \\n\\n **Query**: {query}\"\n",
        "\n",
        "import cohere\n",
        "\n",
        "stream = co.chat_stream(\n",
        "  model='command-r-plus-08-2024',\n",
        "  message=prompt,\n",
        "  temperature=0.4,\n",
        "  chat_history=[],\n",
        "  prompt_truncation='AUTO',\n",
        "  #connectors=[{\"id\":\"web-search\"}],\n",
        "  max_tokens=4096\n",
        ")\n",
        "\n",
        "for event in stream:\n",
        "  if event.event_type == \"text-generation\":\n",
        "    print(event.text, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HBMAl7oRG1T",
        "outputId": "3430fc7f-de56-4367-82fc-1976fe945012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The application of Large Language Models (LLMs) in enterprise solutions offers a wide range of possibilities and benefits, as outlined in the provided context. Here are some key ways LLMs can be utilized in enterprise settings:\n",
            "\n",
            "- **Private Generalized LLM API**: This approach focuses on data privacy, customization, and control. By developing a private LLM API, enterprises can create tailored language models that cater to their specific industry or use case. This allows for better control over sensitive data, ensuring privacy and security. Enterprises can use this to build applications for customer support, content generation, or personalized recommendations, ensuring that the model aligns with their unique requirements.\n",
            "\n",
            "- **LLMs for Enterprise and LLM Ops**: Integrating LLMs into enterprise operations can revolutionize various processes. For instance, LLMs can be used for automated customer support, generating personalized responses to inquiries, and handling a vast array of customer interactions. They can also assist in content creation, generating marketing materials, product descriptions, or even entire articles, thus increasing productivity and reducing costs. Moreover, LLMs can analyze vast amounts of enterprise data, providing valuable insights for decision-making and strategic planning.\n",
            "\n",
            "- **Leveraging Embeddings**: Embeddings, which are numerical representations of words and documents, play a crucial role in enhancing LLMs' semantic understanding. By integrating embeddings from sources like Cohere, OpenAI, and Hugging Face, enterprises can improve the context comprehension, relationship identification, and overall effectiveness of their LLMs. This is particularly useful for tasks like sentiment analysis, document classification, and information retrieval.\n",
            "\n",
            "- **Vector Databases and Semantic Search**: Vector databases are an essential component for optimizing LLMs in enterprise applications. These databases store and index data in a high-dimensional vector space, enabling efficient semantic search capabilities. By integrating vector databases, enterprises can enhance the performance of LLMs in tasks such as document retrieval, question-answering, and content recommendation. This technology ensures that the LLMs can quickly and accurately process and understand large volumes of unstructured data.\n",
            "\n",
            "- **LLMs and Transformers**: The chapter on LLMs and Transformers highlights the power of language models and the Transformer architecture. Transformers, such as the GPT (Generative Pre-trained Transformer) models, have revolutionized natural language processing tasks. Enterprises can utilize these models for various applications, including language translation, text summarization, and dialogue generation. The Transformer architecture's ability to handle sequential data and capture long-range dependencies makes it particularly useful for understanding and generating human-like text.\n",
            "\n",
            "By implementing these strategies, enterprises can harness the power of LLMs to improve operational efficiency, enhance customer experiences, and drive innovation. These models can process and analyze vast amounts of data, providing valuable insights and enabling businesses to make data-driven decisions. Moreover, the customization and control offered by private LLM APIs ensure that enterprises can adapt these technologies to their specific needs, fostering a competitive advantage in the market.\n",
            "\n",
            "As the field of LLMs continues to evolve, enterprises have the opportunity to stay at the forefront of innovation, leveraging these advanced language models to transform their operations and stay ahead in an increasingly competitive business landscape."
          ]
        }
      ],
      "source": [
        "query = \"How can Large Language Models (LLMs) be applied in enterprise solutions?\"\n",
        "context = retrieve(query, index)\n",
        "\n",
        "contexts = \"\"\n",
        "for cont in context:\n",
        "  contexts = contexts + cont\n",
        "prompt = f\"**Context/Knowledge**: {contexts} \\n\\n **Query**: {query}\"\n",
        "\n",
        "import cohere\n",
        "\n",
        "stream = co.chat_stream(\n",
        "  model='command-r-plus-08-2024',\n",
        "  message=prompt,\n",
        "  temperature=0.4,\n",
        "  chat_history=[],\n",
        "  prompt_truncation='AUTO',\n",
        "  #connectors=[{\"id\":\"web-search\"}],\n",
        "  max_tokens=4096\n",
        ")\n",
        "\n",
        "for event in stream:\n",
        "  if event.event_type == \"text-generation\":\n",
        "    print(event.text, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5kwsthIRISa",
        "outputId": "35b1db86-6607-4f19-cdbc-10557f694306"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Transformer architecture has brought about significant advancements in the field of natural language processing (NLP) and has several advantages:\n",
            "\n",
            "**Benefits of Transformer Architecture:**\n",
            "1. **Parallel Processing and Efficiency:** One of the key strengths of the Transformer is its ability to process input sequences in parallel. Unlike traditional sequential models, which process data step by step, the Transformer can handle all input elements simultaneously. This parallel processing capability leads to faster training and inference times, making it highly efficient for large-scale language tasks.\n",
            "2. **Attention Mechanism:** The Transformer's attention mechanism is a powerful tool that allows the model to focus on relevant parts of the input sequence. It assigns attention weights to different elements, enabling the model to weigh the importance of each word or token in the context. This mechanism helps the model capture long-range dependencies and understand the relationships between distant words, which is crucial for tasks like machine translation and text summarization.\n",
            "3. **Interpretability:** The attention weights in the Transformer can be visualized, providing valuable insights into the model's decision-making process. Researchers and developers can analyze which parts of the input are given more attention, making it easier to interpret and debug the model's behavior. This interpretability is a significant advantage for understanding and improving the model's performance.\n",
            "4. **Global Context Awareness:** The Transformer architecture can consider the entire input sequence at once, allowing it to capture global context. This capability is particularly beneficial for tasks that require understanding the relationships between words across long sentences or paragraphs. It can capture long-range dependencies, which was a limitation in previous models that relied on fixed-length contexts.\n",
            "5. **State-of-the-Art Performance:** Transformer-based models have consistently achieved state-of-the-art results in various NLP tasks, including machine translation, text generation, language understanding, and more. They have set new benchmarks and outperformed traditional models, demonstrating their effectiveness and versatility.\n",
            "\n",
            "**Limitations of Transformer Architecture:**\n",
            "1. **Fixed Input Length:** The Transformer architecture typically requires fixed-length input sequences due to the use of positional encodings. Handling variable-length sequences can be challenging and may require additional preprocessing steps or padding. This limitation can make it less flexible for tasks with dynamic input lengths.\n",
            "2. **Computational Complexity:** While the Transformer is efficient in parallel processing, it can still be computationally expensive, especially for very long sequences. The self-attention mechanism, which is at the core of the Transformer, has a time complexity of O(n^2) with respect to the sequence length, which can become a bottleneck for extremely long inputs.\n",
            "3. **Memory Requirements:** Transformer models often have a large number of parameters, leading to high memory requirements. Training and deploying such models can be resource-intensive, especially for hardware with limited memory capacity.\n",
            "4. **Training Data Requirements:** To achieve optimal performance, Transformer models typically require vast amounts of training data. Pre-training on large-scale datasets is common, and the quality and diversity of the training data significantly impact the model's performance. Obtaining and processing such extensive datasets can be a challenge.\n",
            "5. **Overfitting:** Due to their large number of parameters, Transformer models are susceptible to overfitting, especially when trained on smaller datasets. Regularization techniques and careful hyperparameter tuning are essential to mitigate this issue.\n",
            "\n",
            "Despite these limitations, the Transformer architecture has proven to be a powerful and versatile approach, driving significant progress in NLP. Ongoing research aims to address these limitations and further improve the capabilities of Transformer-based models."
          ]
        }
      ],
      "source": [
        "query = \"What are the benefits and limitations of the Transformer architecture?\"\n",
        "context = retrieve(query, index)\n",
        "\n",
        "contexts = \"\"\n",
        "for cont in context:\n",
        "  contexts = contexts + cont\n",
        "prompt = f\"**Context/Knowledge**: {contexts} \\n\\n **Query**: {query}\"\n",
        "\n",
        "import cohere\n",
        "\n",
        "stream = co.chat_stream(\n",
        "  model='command-r-plus-08-2024',\n",
        "  message=prompt,\n",
        "  temperature=0.4,\n",
        "  chat_history=[],\n",
        "  prompt_truncation='AUTO',\n",
        "  #connectors=[{\"id\":\"web-search\"}],\n",
        "  max_tokens=4096\n",
        ")\n",
        "\n",
        "for event in stream:\n",
        "  if event.event_type == \"text-generation\":\n",
        "    print(event.text, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EngZYx6RH86",
        "outputId": "5d6d54ef-8c25-4560-f750-d848d737105e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The attention mechanism in Transformer models is a crucial component that enables the model to focus on relevant parts of the input sequence and capture important dependencies and relationships between elements. Here's a detailed explanation of how it works:\n",
            "\n",
            "**1. Scaled Dot-Product Attention:** The attention mechanism used in Transformers is often the Scaled Dot-Product Attention. This process involves the following steps:\n",
            "   - **Query, Key, and Value Vectors:** The input to the attention mechanism is a set of vectors: the query vector (Q), key vectors (K), and value vectors (V). In the context of the Transformer, these vectors are derived from the input embeddings and are learned during the training process.\n",
            "   - **Dot Product:** The attention weights are calculated by taking the dot product between the query vector and each key vector. The dot product measures the similarity between the query and each key. Higher dot products indicate higher similarity.\n",
            "   - **Scaling:** To prevent extremely large values in the dot product, especially with large vector dimensions, the result is scaled by dividing it by the square root of the dimension of the key vectors.\n",
            "   - **Softmax Function:** The scaled dot products are then passed through a softmax function to obtain the attention weights. The softmax converts the scaled dot products into a probability distribution, ensuring that the weights sum up to 1. This means that the model can focus on the most relevant parts of the input.\n",
            "\n",
            "**2. Multi-Head Attention:** Transformer models use multi-head attention, which means they perform the above process multiple times in parallel, with different sets of weight matrices. Each of these parallel executions is called an \"attention head.\" Here's how it works:\n",
            "   - **Multiple Sets of Q, K, and V:** For each attention head, the input vectors (Q, K, and V) are linearly projected onto different subspaces using distinct weight matrices. This allows each head to capture different aspects or representations of the input.\n",
            "   - **Concatenation and Final Output:** The outputs of each attention head are concatenated and then transformed back to the original dimension using another weight matrix. This multi-head approach enables the model to jointly attend to information from different representation subspaces, providing a more comprehensive understanding of the input.\n",
            "\n",
            "**3. Encoder-Decoder Attention:** In the context of the Transformer architecture, there are three main types of attention mechanisms:\n",
            " - **Self-Attention (within the Encoder):** In the encoder, self-attention allows each word or token to attend to all other words in the input sequence, capturing relationships and dependencies.\n",
            " - **Self-Attention (within the Decoder):** Similarly, the decoder also uses self-attention, but it is modified to prevent attending to future tokens, ensuring the model generates output based only on previous context.\n",
            " - **Encoder-Decoder Attention:** This type of attention connects the encoder and decoder. The queries come from the previous decoder layer, and the keys and values are derived from the encoder's output. This allows the decoder to focus on relevant parts of the input sequence during the decoding process, facilitating better context understanding.\n",
            "\n",
            "**4. Weighted Sum of Values:** The attention weights obtained from the attention mechanism are used to calculate a weighted sum of the value vectors (V). Each value vector is multiplied by its corresponding attention weight, and these weighted vectors are summed to produce the final output vector. This output vector represents the relevant and important information from the input sequence, as determined by the attention mechanism.\n",
            "\n",
            "**5. Contextual Understanding and Alignment:** The attention mechanism in Transformers allows the model to align the input sequence with the output sequence, ensuring that the model generates coherent and contextually appropriate responses. It enables the model to understand the relationships between input elements and make informed predictions.\n",
            "\n",
            "Overall, the attention mechanism in Transformer models provides a dynamic and flexible way to weigh and combine information from different parts of the input sequence, allowing the model to focus on the most pertinent details for each specific task. This mechanism is a significant contributor to the Transformer's success in various natural language processing tasks, especially those involving long-range dependencies and complex contextual relationships."
          ]
        }
      ],
      "source": [
        "query = \"How does the attention mechanism in Transformer models work?\"\n",
        "context = retrieve(query, index)\n",
        "\n",
        "contexts = \"\"\n",
        "for cont in context:\n",
        "  contexts = contexts + cont\n",
        "prompt = f\"**Context/Knowledge**: {contexts} \\n\\n **Query**: {query}\"\n",
        "\n",
        "import cohere\n",
        "\n",
        "stream = co.chat_stream(\n",
        "  model='command-r-plus-08-2024',\n",
        "  message=prompt,\n",
        "  temperature=0.4,\n",
        "  chat_history=[],\n",
        "  prompt_truncation='AUTO',\n",
        "  #connectors=[{\"id\":\"web-search\"}],\n",
        "  max_tokens=4096\n",
        ")\n",
        "\n",
        "for event in stream:\n",
        "  if event.event_type == \"text-generation\":\n",
        "    print(event.text, end='')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
