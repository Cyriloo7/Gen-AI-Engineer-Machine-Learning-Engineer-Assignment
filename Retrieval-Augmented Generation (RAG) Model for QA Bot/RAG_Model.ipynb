{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGVHTJP_j4MH"
      },
      "source": [
        "# Part 1: Retrieval-Augmented Generation (RAG) Model for QA Bot\n",
        "\n",
        "### Problem Statement:\n",
        "\n",
        "### Develop a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA)\n",
        "\n",
        "### bot for a business. Use a vector database like Pinecone DB and a generative model like\n",
        "\n",
        "### Cohere API (or any other available alternative). The QA bot should be able to retrieve\n",
        "\n",
        "### relevant information from a dataset and generate coherent answers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation Requirements"
      ],
      "metadata": {
        "id": "tv_IQZ_KIik9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu cohere PyPDF2 numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyvYUYiT8zP6",
        "outputId": "0b270aae-4159-439f-c7ac-6e4533fc59a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting cohere\n",
            "  Downloading cohere-5.9.2-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Collecting boto3<2.0.0,>=1.34.0 (from cohere)\n",
            "  Downloading boto3-1.35.19-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx>=0.21.2 (from cohere)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.9.1)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.23.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.19.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20240914-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.12.2)\n",
            "Collecting botocore<1.36.0,>=1.35.19 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading botocore-1.35.19-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.21.2->cohere)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.21.2->cohere)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.0.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15->cohere) (0.24.6)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.19->boto3<2.0.0,>=1.34.0->cohere) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.66.5)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.19->boto3<2.0.0,>=1.34.0->cohere) (1.16.0)\n",
            "Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.9.2-py3-none-any.whl (222 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.4/222.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.35.19-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading types_requests-2.32.0.20240914-py3-none-any.whl (15 kB)\n",
            "Downloading botocore-1.35.19-py3-none-any.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: types-requests, PyPDF2, parameterized, jmespath, httpx-sse, h11, fastavro, faiss-cpu, httpcore, botocore, s3transfer, httpx, boto3, cohere\n",
            "Successfully installed PyPDF2-3.0.1 boto3-1.35.19 botocore-1.35.19 cohere-5.9.2 faiss-cpu-1.8.0.post1 fastavro-1.9.7 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 httpx-sse-0.4.0 jmespath-1.0.1 parameterized-0.9.0 s3transfer-0.10.2 types-requests-2.32.0.20240914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read contexts from PDF file"
      ],
      "metadata": {
        "id": "AB5yWxwqIhI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "N1Ag3mUJ80Zi"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spliting the context into chunks"
      ],
      "metadata": {
        "id": "LBJKm-BRIt-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text, chunk_size=1000):\n",
        "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "AyP9OUzn864r"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Cohere API services for Text Embeding."
      ],
      "metadata": {
        "id": "rLGsH7d5KqMS"
      }
    },
    {
      "source": [
        "import cohere\n",
        "import numpy as np\n",
        "\n",
        "co = cohere.Client('SGXUJ2vUDqaNNpJwh1ffmo1PFkGmN50W6ghcW4UA')\n",
        "\n",
        "def create_embeddings(texts, batch_size=40):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "\n",
        "        batch = texts[i:i+batch_size]\n",
        "        response = co.embed(texts=batch, model=\"embed-english-v3.0\", input_type=\"search_document\")\n",
        "        embeddings.append(response.embeddings)\n",
        "\n",
        "    return np.vstack(embeddings)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "U8MqPnoi-zeI"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing FAISS Indexing"
      ],
      "metadata": {
        "id": "HB5T_i-YLJvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "dimension = 1024\n",
        "index = faiss.IndexFlatL2(dimension)\n"
      ],
      "metadata": {
        "id": "vAswz5XG9Muz"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing: Reading a pdf file, Extracting text from that PDF, Spliting into chunks of data, Vector Embeding and Converting the embeded data into FAISS Index"
      ],
      "metadata": {
        "id": "QWQb3orCLPmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = '/content/Applied-Generative-AI-for-Beginners.pdf'\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "chunks = split_text(text)\n",
        "\n",
        "# Generate embeddings\n",
        "chunk_embeddings = create_embeddings(chunks)\n",
        "\n",
        "# Add embeddings to FAISS index\n",
        "index.add(np.array(chunk_embeddings).astype(np.float32))\n"
      ],
      "metadata": {
        "id": "JoE3O3SH9QAC"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to retrive relevent information from the documents."
      ],
      "metadata": {
        "id": "6Mw3xG69Ltlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, index, k=3):\n",
        "\n",
        "    query_embed = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_document\").embeddings\n",
        "\n",
        "    D, I = index.search(np.array(query_embed).astype(np.float32), k)\n",
        "\n",
        "    return [chunks[i] for i in I[0]]"
      ],
      "metadata": {
        "id": "d44tq_me9lpR"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation of the text from prompt"
      ],
      "metadata": {
        "id": "zsuc1MC1MbpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Generative AI and its applications?\"\n",
        "context = retrieve(query, index)\n",
        "\n",
        "contexts = \"\"\n",
        "for cont in context:\n",
        "  contexts = contexts + cont\n",
        "\n",
        "prompt = f\"**Context/Knowledge**: {contexts} \\n\\n **Query**: {query}\""
      ],
      "metadata": {
        "id": "uBDw4ND-9n5Z"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming the output."
      ],
      "metadata": {
        "id": "lSzoA-tdMmSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "\n",
        "stream = co.chat_stream(\n",
        "  model='command-r-plus-08-2024',\n",
        "  message=prompt,\n",
        "  temperature=0.4,\n",
        "  chat_history=[],\n",
        "  prompt_truncation='AUTO',\n",
        "  #connectors=[{\"id\":\"web-search\"}],\n",
        "  max_tokens=4096\n",
        ")\n",
        "\n",
        "for event in stream:\n",
        "  if event.event_type == \"text-generation\":\n",
        "    print(event.text, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "annIc2CgB1od",
        "outputId": "9cdd2134-2787-49ee-dcd7-e29ed175e4b4"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Generative AI:\n",
            "Generative AI is a fascinating and rapidly evolving field within artificial intelligence. It involves the development of advanced algorithms and models that can create new and diverse content, mimicking the creative process of humans. Unlike traditional AI systems that are designed for specific tasks, generative AI focuses on learning patterns and structures from existing data to produce novel outputs.\n",
            "\n",
            "## Applications of Generative AI:\n",
            "1. **Text Generation**:\n",
            "   - Generative AI models can write creative stories, news articles, poetry, and even code. For example, GPT (Generative Pre-trained Transformer) models have gained significant attention for their ability to generate coherent and contextually relevant text.\n",
            "   - These models can assist content creators, writers, and marketers in generating ideas, outlines, and drafts, thereby increasing productivity.\n",
            "\n",
            "2. **Image Generation**:\n",
            "   - Generative AI can create realistic images, artwork, and even modify or enhance existing visuals. Models like Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) are widely used for this purpose.\n",
            "   - Applications include generating product images for e-commerce, creating virtual avatars, and enhancing image quality in photography and graphic design.\n",
            "\n",
            "3. **Audio Generation**:\n",
            "   - This technology can synthesize speech, music, and sound effects. WaveNet, a deep generative model for raw audio waveforms, has been used to generate human-like speech.\n",
            "   - Applications range from creating personalized voice assistants to composing music and generating sound effects for games and movies.\n",
            "\n",
            "4. **Video Generation**:\n",
            "   - Generative AI models can create videos, animations, and simulations. This includes generating realistic human actions, synthesizing video content, and even creating virtual environments.\n",
            "   - It has applications in the film and gaming industries, virtual reality, and content creation for various media platforms.\n",
            "\n",
            "5. **Data Augmentation**:\n",
            "   - Generative models can be used to create synthetic data for training other AI systems, especially in cases where real-world data is limited.\n",
            "   - This is particularly useful in fields like medical imaging, where generating diverse and realistic medical data can improve diagnostic models.\n",
            "\n",
            "6. **Creative Content Generation**:\n",
            "   - Generative AI is revolutionizing the creative industries. It can assist in generating artwork, designing fashion collections, creating marketing content, and even composing music for various genres.\n",
            "   - These models can inspire and collaborate with human artists, offering new avenues for creative expression.\n",
            "\n",
            "7. **Personalization and Recommendations**:\n",
            "   - Generative AI can be used to provide personalized recommendations in e-commerce, entertainment, and content platforms. By understanding user preferences, it can generate tailored suggestions.\n",
            "\n",
            "8. **Language Translation and Localization**:\n",
            "   - Generative models can assist in machine translation tasks, generating translations that sound more natural and fluent.\n",
            "   - This is valuable for businesses aiming to localize their content for global audiences.\n",
            "\n",
            "The applications of Generative AI are vast and continue to expand across various industries. As the technology advances, we can expect even more innovative use cases, blurring the lines between human creativity and artificial intelligence."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Provide several example queries**"
      ],
      "metadata": {
        "id": "NK1UH5GPQcor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How do diffusion models work in generating images?\"\n",
        "context = retrieve(query, index)\n",
        "\n",
        "contexts = \"\"\n",
        "for cont in context:\n",
        "  contexts = contexts + cont\n",
        "prompt = f\"**Context/Knowledge**: {contexts} \\n\\n **Query**: {query}\"\n",
        "\n",
        "import cohere\n",
        "\n",
        "stream = co.chat_stream(\n",
        "  model='command-r-plus-08-2024',\n",
        "  message=prompt,\n",
        "  temperature=0.4,\n",
        "  chat_history=[],\n",
        "  prompt_truncation='AUTO',\n",
        "  #connectors=[{\"id\":\"web-search\"}],\n",
        "  max_tokens=4096\n",
        ")\n",
        "\n",
        "for event in stream:\n",
        "  if event.event_type == \"text-generation\":\n",
        "    print(event.text, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAZ_aOQNQajU",
        "outputId": "8ef9113e-a711-4aa1-bce6-70cda902939f"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diffusion models offer a unique and innovative approach to image generation by employing a process that can be likened to a series of steps, each adding a layer of complexity to the image generation process. Here's a breakdown of how diffusion models generate images:\n",
            "\n",
            "**1. Noise Schedule and Markov Chain:** The process begins with the definition of a noise schedule, which is a sequence of noise levels ranging from minimal to significant. This schedule is crucial as it determines the progression of noise introduction. The model then employs a Markov chain, a sequential process where each step corresponds to a noise level in the schedule. \n",
            "\n",
            "**2. Adding Noise and Latent Representation:** At each step of the Markov chain, the model introduces noise to the image. This is a controlled process, where the amount of noise added is determined by the diffusion rate. Simultaneously, the model also uses a latent representation model, typically a neural network, to encode the image into a latent representation. This latent representation captures the essential features and structure of the image.\n",
            "\n",
            "**3. Iterative Process and Parameter Optimization:** The diffusion process is iterative, meaning it repeats the steps of adding noise and updating the latent representation. During this process, the model aims to find the optimal parameters for the latent representation model. These parameters are adjusted to maximize the likelihood that the model could have generated the real images in the dataset. In other words, the model learns to reverse the noise addition process, gradually refining the image quality.\n",
            "\n",
            "**4. Diffusion Process and Image Generation:** The diffusion process is a probabilistic, state-transition process that generates new images. It starts with a latent representation and, over multiple steps, modifies it by adding and refining noise. The diffusion rate controls the amount of noise added at each step. As the process progresses, the model generates images that become increasingly detailed and realistic, aiming to match the quality of real images.\n",
            "\n",
            "**5. Applications:** Diffusion models have various applications in image generation, including:\n",
            "- **Image Synthesis from Text:** These models can generate images based on textual descriptions, making them useful for text-to-image applications.\n",
            "- **Style Transfer:** They can transfer the style of one image to another, allowing for creative image manipulations.\n",
            "- **Super-resolution:** Diffusion models can enhance the resolution of low-resolution images, improving their visual quality.\n",
            "\n",
            "The power of diffusion models lies in their ability to start with random noise and gradually transform it into a structured, meaningful image. This process is guided by the noise schedule, the latent representation model, and the iterative optimization of parameters. As a result, diffusion models can generate diverse and high-quality images, making them a promising tool in the field of generative AI for images."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the architecture of ChatGPT and how is it fine-tuned?\"\n",
        "context = retrieve(query, index)\n",
        "\n",
        "contexts = \"\"\n",
        "for cont in context:\n",
        "  contexts = contexts + cont\n",
        "prompt = f\"**Context/Knowledge**: {contexts} \\n\\n **Query**: {query}\"\n",
        "\n",
        "import cohere\n",
        "\n",
        "stream = co.chat_stream(\n",
        "  model='command-r-plus-08-2024',\n",
        "  message=prompt,\n",
        "  temperature=0.4,\n",
        "  chat_history=[],\n",
        "  prompt_truncation='AUTO',\n",
        "  #connectors=[{\"id\":\"web-search\"}],\n",
        "  max_tokens=4096\n",
        ")\n",
        "\n",
        "for event in stream:\n",
        "  if event.event_type == \"text-generation\":\n",
        "    print(event.text, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2XUvvrhQ6kF",
        "outputId": "91369223-986b-417f-e6f2-2fbd8417b6db"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The architecture of ChatGPT is based on the Transformer model, a powerful neural network architecture initially introduced by Vaswani et al. in 2017. Specifically, ChatGPT utilizes a \"decoder-only\" version of the Transformer, which is well-suited for language generation tasks. The Transformer architecture consists of an encoder and a decoder, but in the case of ChatGPT, only the decoder component is used.\n",
            "\n",
            "**Architecture Components**:\n",
            "- **Decoder-Only Transformer**: The decoder in the Transformer architecture is responsible for generating output sequences. It takes an input and generates a corresponding response. In ChatGPT, the decoder is trained to produce coherent and contextually appropriate responses to user queries.\n",
            "- **Attention Mechanism**: ChatGPT employs the self-attention mechanism, a key feature of the Transformer architecture. This mechanism allows the model to weigh the importance of different parts of the input sequence when generating a response. It enables the model to capture long-range dependencies and generate contextually relevant outputs.\n",
            "- **Reinforcement Learning from Human Feedback (RLHF)**: This is a significant component of ChatGPT's architecture and training process. RLHF is a technique where the model is fine-tuned using human feedback to improve its responses. Human evaluators provide feedback on the model's responses, and this feedback is used to guide the model's learning process, encouraging more human-like and helpful interactions.\n",
            "\n",
            "**Fine-Tuning Process**:\n",
            "Fine-tuning is a crucial step in adapting the pre-trained ChatGPT model to specific tasks and domains, ensuring it provides relevant and accurate responses. Here's an overview of the fine-tuning process:\n",
            "1. **Pre-training**: Before fine-tuning, ChatGPT undergoes a pre-training phase where it learns general language patterns and representations from a vast amount of text data. This pre-training is typically done using unsupervised learning on a large corpus of text.\n",
            "2. **Domain Adaptation**: During fine-tuning, ChatGPT is exposed to domain-specific datasets relevant to the intended use case. For example, if ChatGPT is to be used for customer support, it will be fine-tuned on customer service conversations and queries. This step helps the model adapt its knowledge and language understanding to the specific domain.\n",
            "3. **User Interaction Guidance**: Fine-tuning also involves training the model to interact with users appropriately. This is achieved through reinforcement learning from human feedback (RLHF). Human evaluators provide feedback on the model's responses, rewarding desired behaviors and penalizing inappropriate or harmful responses. This process helps ChatGPT learn to generate contextually relevant, helpful, and safe outputs.\n",
            "4. **Iterative Improvement**: Fine-tuning is an iterative process. The model's performance is continuously evaluated, and adjustments are made to improve its responses. This may involve further exposure to domain-specific data, refining the RLHF process, or optimizing hyperparameters.\n",
            "5. **Contextual Embeddings**: ChatGPT uses contextual embeddings to represent user inputs and generate responses. These embeddings capture the semantic meaning of words in context, allowing the model to understand and generate coherent text.\n",
            "6. **Response Generation**: Fine-tuning teaches ChatGPT to generate responses by predicting the next word or token in a sequence, given the previous context. It learns to do this in a way that is consistent with the desired task and user expectations.\n",
            "\n",
            "The fine-tuning process allows ChatGPT to adapt to various applications, such as customer support, language translation, content generation, and more. By combining pre-training with domain-specific fine-tuning and user interaction guidance, ChatGPT can provide contextually relevant and helpful responses in interactive conversations."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the key differences between Google Bard and ChatGPT?\"\n",
        "context = retrieve(query, index)\n",
        "\n",
        "contexts = \"\"\n",
        "for cont in context:\n",
        "  contexts = contexts + cont\n",
        "prompt = f\"**Context/Knowledge**: {contexts} \\n\\n **Query**: {query}\"\n",
        "\n",
        "import cohere\n",
        "\n",
        "stream = co.chat_stream(\n",
        "  model='command-r-plus-08-2024',\n",
        "  message=prompt,\n",
        "  temperature=0.4,\n",
        "  chat_history=[],\n",
        "  prompt_truncation='AUTO',\n",
        "  #connectors=[{\"id\":\"web-search\"}],\n",
        "  max_tokens=4096\n",
        ")\n",
        "\n",
        "for event in stream:\n",
        "  if event.event_type == \"text-generation\":\n",
        "    print(event.text, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1WW_44cRAoM",
        "outputId": "fef1c040-4e09-41b7-843e-1fabe756af2a"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The key differences between Google Bard and ChatGPT can be summarized as follows:\n",
            "\n",
            "**Architecture**: The most significant distinction lies in their architectural design. ChatGPT employs a decoder-only architecture, which means it is optimized for generating text. It takes input and generates a response based on that input. On the other hand, Google Bard utilizes an encoder-decoder architecture. This architecture allows Bard to both encode input and decode it to generate a response. The encoder-decoder setup enables Bard to handle tasks that require understanding and processing the input before generating an output.\n",
            "\n",
            "**Capabilities**: Both models are large language models with impressive capabilities, but they excel in different areas. ChatGPT, with its decoder-only architecture, is particularly skilled at generating text, making it excellent for tasks like language translation, summarization, and creative writing. Google Bard, however, is better at tasks that require real-world knowledge and understanding. Bard can answer questions, provide informative responses, and generate creative content by drawing upon its vast knowledge base. This makes Bard more suitable for tasks that demand factual accuracy and real-world context.\n",
            "\n",
            "**Performance**: The performance comparison between the two models may vary depending on the task. In general, ChatGPT's strength lies in its ability to generate coherent and fluent text, making it a powerful tool for language-related tasks. Google Bard, with its access to a vast amount of information, often provides more accurate and informative responses, especially for queries that require factual knowledge.\n",
            "\n",
            "**Bias**: Bias is an important consideration with any language model. The AI Ethics Team at Google AI conducted a bias comparison between ChatGPT and Bard. The results of this comparison are not explicitly mentioned in the provided context, but it is crucial to evaluate and address any potential biases in these models to ensure ethical and unbiased text generation.\n",
            "\n",
            "**Use Cases**: The choice between ChatGPT and Google Bard depends on the specific use case. For applications that require extensive text generation, creative writing, or language translation, ChatGPT might be more suitable. On the other hand, Google Bard is a better fit for tasks like answering questions, providing information, or generating content that requires real-world context and factual accuracy.\n",
            "\n",
            "The sources provided in the context offer more detailed comparisons and insights into the performance and capabilities of both models, which can be valuable for a comprehensive understanding of their differences."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How can Large Language Models (LLMs) be applied in enterprise solutions?\"\n",
        "context = retrieve(query, index)\n",
        "\n",
        "contexts = \"\"\n",
        "for cont in context:\n",
        "  contexts = contexts + cont\n",
        "prompt = f\"**Context/Knowledge**: {contexts} \\n\\n **Query**: {query}\"\n",
        "\n",
        "import cohere\n",
        "\n",
        "stream = co.chat_stream(\n",
        "  model='command-r-plus-08-2024',\n",
        "  message=prompt,\n",
        "  temperature=0.4,\n",
        "  chat_history=[],\n",
        "  prompt_truncation='AUTO',\n",
        "  #connectors=[{\"id\":\"web-search\"}],\n",
        "  max_tokens=4096\n",
        ")\n",
        "\n",
        "for event in stream:\n",
        "  if event.event_type == \"text-generation\":\n",
        "    print(event.text, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HBMAl7oRG1T",
        "outputId": "3430fc7f-de56-4367-82fc-1976fe945012"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The application of Large Language Models (LLMs) in enterprise solutions offers a wide range of possibilities and benefits, as outlined in the provided context. Here are some key ways LLMs can be utilized in enterprise settings:\n",
            "\n",
            "- **Private Generalized LLM API**: This approach focuses on data privacy, customization, and control. By developing a private LLM API, enterprises can create tailored language models that cater to their specific industry or use case. This allows for better control over sensitive data, ensuring privacy and security. Enterprises can use this to build applications for customer support, content generation, or personalized recommendations, ensuring that the model aligns with their unique requirements.\n",
            "\n",
            "- **LLMs for Enterprise and LLM Ops**: Integrating LLMs into enterprise operations can revolutionize various processes. For instance, LLMs can be used for automated customer support, generating personalized responses to inquiries, and handling a vast array of customer interactions. They can also assist in content creation, generating marketing materials, product descriptions, or even entire articles, thus increasing productivity and reducing costs. Moreover, LLMs can analyze vast amounts of enterprise data, providing valuable insights for decision-making and strategic planning.\n",
            "\n",
            "- **Leveraging Embeddings**: Embeddings, which are numerical representations of words and documents, play a crucial role in enhancing LLMs' semantic understanding. By integrating embeddings from sources like Cohere, OpenAI, and Hugging Face, enterprises can improve the context comprehension, relationship identification, and overall effectiveness of their LLMs. This is particularly useful for tasks like sentiment analysis, document classification, and information retrieval.\n",
            "\n",
            "- **Vector Databases and Semantic Search**: Vector databases are an essential component for optimizing LLMs in enterprise applications. These databases store and index data in a high-dimensional vector space, enabling efficient semantic search capabilities. By integrating vector databases, enterprises can enhance the performance of LLMs in tasks such as document retrieval, question-answering, and content recommendation. This technology ensures that the LLMs can quickly and accurately process and understand large volumes of unstructured data.\n",
            "\n",
            "- **LLMs and Transformers**: The chapter on LLMs and Transformers highlights the power of language models and the Transformer architecture. Transformers, such as the GPT (Generative Pre-trained Transformer) models, have revolutionized natural language processing tasks. Enterprises can utilize these models for various applications, including language translation, text summarization, and dialogue generation. The Transformer architecture's ability to handle sequential data and capture long-range dependencies makes it particularly useful for understanding and generating human-like text.\n",
            "\n",
            "By implementing these strategies, enterprises can harness the power of LLMs to improve operational efficiency, enhance customer experiences, and drive innovation. These models can process and analyze vast amounts of data, providing valuable insights and enabling businesses to make data-driven decisions. Moreover, the customization and control offered by private LLM APIs ensure that enterprises can adapt these technologies to their specific needs, fostering a competitive advantage in the market.\n",
            "\n",
            "As the field of LLMs continues to evolve, enterprises have the opportunity to stay at the forefront of innovation, leveraging these advanced language models to transform their operations and stay ahead in an increasingly competitive business landscape."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the benefits and limitations of the Transformer architecture?\"\n",
        "context = retrieve(query, index)\n",
        "\n",
        "contexts = \"\"\n",
        "for cont in context:\n",
        "  contexts = contexts + cont\n",
        "prompt = f\"**Context/Knowledge**: {contexts} \\n\\n **Query**: {query}\"\n",
        "\n",
        "import cohere\n",
        "\n",
        "stream = co.chat_stream(\n",
        "  model='command-r-plus-08-2024',\n",
        "  message=prompt,\n",
        "  temperature=0.4,\n",
        "  chat_history=[],\n",
        "  prompt_truncation='AUTO',\n",
        "  #connectors=[{\"id\":\"web-search\"}],\n",
        "  max_tokens=4096\n",
        ")\n",
        "\n",
        "for event in stream:\n",
        "  if event.event_type == \"text-generation\":\n",
        "    print(event.text, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5kwsthIRISa",
        "outputId": "35b1db86-6607-4f19-cdbc-10557f694306"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Transformer architecture has brought about significant advancements in the field of natural language processing (NLP) and has several advantages:\n",
            "\n",
            "**Benefits of Transformer Architecture:**\n",
            "1. **Parallel Processing and Efficiency:** One of the key strengths of the Transformer is its ability to process input sequences in parallel. Unlike traditional sequential models, which process data step by step, the Transformer can handle all input elements simultaneously. This parallel processing capability leads to faster training and inference times, making it highly efficient for large-scale language tasks.\n",
            "2. **Attention Mechanism:** The Transformer's attention mechanism is a powerful tool that allows the model to focus on relevant parts of the input sequence. It assigns attention weights to different elements, enabling the model to weigh the importance of each word or token in the context. This mechanism helps the model capture long-range dependencies and understand the relationships between distant words, which is crucial for tasks like machine translation and text summarization.\n",
            "3. **Interpretability:** The attention weights in the Transformer can be visualized, providing valuable insights into the model's decision-making process. Researchers and developers can analyze which parts of the input are given more attention, making it easier to interpret and debug the model's behavior. This interpretability is a significant advantage for understanding and improving the model's performance.\n",
            "4. **Global Context Awareness:** The Transformer architecture can consider the entire input sequence at once, allowing it to capture global context. This capability is particularly beneficial for tasks that require understanding the relationships between words across long sentences or paragraphs. It can capture long-range dependencies, which was a limitation in previous models that relied on fixed-length contexts.\n",
            "5. **State-of-the-Art Performance:** Transformer-based models have consistently achieved state-of-the-art results in various NLP tasks, including machine translation, text generation, language understanding, and more. They have set new benchmarks and outperformed traditional models, demonstrating their effectiveness and versatility.\n",
            "\n",
            "**Limitations of Transformer Architecture:**\n",
            "1. **Fixed Input Length:** The Transformer architecture typically requires fixed-length input sequences due to the use of positional encodings. Handling variable-length sequences can be challenging and may require additional preprocessing steps or padding. This limitation can make it less flexible for tasks with dynamic input lengths.\n",
            "2. **Computational Complexity:** While the Transformer is efficient in parallel processing, it can still be computationally expensive, especially for very long sequences. The self-attention mechanism, which is at the core of the Transformer, has a time complexity of O(n^2) with respect to the sequence length, which can become a bottleneck for extremely long inputs.\n",
            "3. **Memory Requirements:** Transformer models often have a large number of parameters, leading to high memory requirements. Training and deploying such models can be resource-intensive, especially for hardware with limited memory capacity.\n",
            "4. **Training Data Requirements:** To achieve optimal performance, Transformer models typically require vast amounts of training data. Pre-training on large-scale datasets is common, and the quality and diversity of the training data significantly impact the model's performance. Obtaining and processing such extensive datasets can be a challenge.\n",
            "5. **Overfitting:** Due to their large number of parameters, Transformer models are susceptible to overfitting, especially when trained on smaller datasets. Regularization techniques and careful hyperparameter tuning are essential to mitigate this issue.\n",
            "\n",
            "Despite these limitations, the Transformer architecture has proven to be a powerful and versatile approach, driving significant progress in NLP. Ongoing research aims to address these limitations and further improve the capabilities of Transformer-based models."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How does the attention mechanism in Transformer models work?\"\n",
        "context = retrieve(query, index)\n",
        "\n",
        "contexts = \"\"\n",
        "for cont in context:\n",
        "  contexts = contexts + cont\n",
        "prompt = f\"**Context/Knowledge**: {contexts} \\n\\n **Query**: {query}\"\n",
        "\n",
        "import cohere\n",
        "\n",
        "stream = co.chat_stream(\n",
        "  model='command-r-plus-08-2024',\n",
        "  message=prompt,\n",
        "  temperature=0.4,\n",
        "  chat_history=[],\n",
        "  prompt_truncation='AUTO',\n",
        "  #connectors=[{\"id\":\"web-search\"}],\n",
        "  max_tokens=4096\n",
        ")\n",
        "\n",
        "for event in stream:\n",
        "  if event.event_type == \"text-generation\":\n",
        "    print(event.text, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EngZYx6RH86",
        "outputId": "5d6d54ef-8c25-4560-f750-d848d737105e"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The attention mechanism in Transformer models is a crucial component that enables the model to focus on relevant parts of the input sequence and capture important dependencies and relationships between elements. Here's a detailed explanation of how it works:\n",
            "\n",
            "**1. Scaled Dot-Product Attention:** The attention mechanism used in Transformers is often the Scaled Dot-Product Attention. This process involves the following steps:\n",
            "   - **Query, Key, and Value Vectors:** The input to the attention mechanism is a set of vectors: the query vector (Q), key vectors (K), and value vectors (V). In the context of the Transformer, these vectors are derived from the input embeddings and are learned during the training process.\n",
            "   - **Dot Product:** The attention weights are calculated by taking the dot product between the query vector and each key vector. The dot product measures the similarity between the query and each key. Higher dot products indicate higher similarity.\n",
            "   - **Scaling:** To prevent extremely large values in the dot product, especially with large vector dimensions, the result is scaled by dividing it by the square root of the dimension of the key vectors.\n",
            "   - **Softmax Function:** The scaled dot products are then passed through a softmax function to obtain the attention weights. The softmax converts the scaled dot products into a probability distribution, ensuring that the weights sum up to 1. This means that the model can focus on the most relevant parts of the input.\n",
            "\n",
            "**2. Multi-Head Attention:** Transformer models use multi-head attention, which means they perform the above process multiple times in parallel, with different sets of weight matrices. Each of these parallel executions is called an \"attention head.\" Here's how it works:\n",
            "   - **Multiple Sets of Q, K, and V:** For each attention head, the input vectors (Q, K, and V) are linearly projected onto different subspaces using distinct weight matrices. This allows each head to capture different aspects or representations of the input.\n",
            "   - **Concatenation and Final Output:** The outputs of each attention head are concatenated and then transformed back to the original dimension using another weight matrix. This multi-head approach enables the model to jointly attend to information from different representation subspaces, providing a more comprehensive understanding of the input.\n",
            "\n",
            "**3. Encoder-Decoder Attention:** In the context of the Transformer architecture, there are three main types of attention mechanisms:\n",
            " - **Self-Attention (within the Encoder):** In the encoder, self-attention allows each word or token to attend to all other words in the input sequence, capturing relationships and dependencies.\n",
            " - **Self-Attention (within the Decoder):** Similarly, the decoder also uses self-attention, but it is modified to prevent attending to future tokens, ensuring the model generates output based only on previous context.\n",
            " - **Encoder-Decoder Attention:** This type of attention connects the encoder and decoder. The queries come from the previous decoder layer, and the keys and values are derived from the encoder's output. This allows the decoder to focus on relevant parts of the input sequence during the decoding process, facilitating better context understanding.\n",
            "\n",
            "**4. Weighted Sum of Values:** The attention weights obtained from the attention mechanism are used to calculate a weighted sum of the value vectors (V). Each value vector is multiplied by its corresponding attention weight, and these weighted vectors are summed to produce the final output vector. This output vector represents the relevant and important information from the input sequence, as determined by the attention mechanism.\n",
            "\n",
            "**5. Contextual Understanding and Alignment:** The attention mechanism in Transformers allows the model to align the input sequence with the output sequence, ensuring that the model generates coherent and contextually appropriate responses. It enables the model to understand the relationships between input elements and make informed predictions.\n",
            "\n",
            "Overall, the attention mechanism in Transformer models provides a dynamic and flexible way to weigh and combine information from different parts of the input sequence, allowing the model to focus on the most pertinent details for each specific task. This mechanism is a significant contributor to the Transformer's success in various natural language processing tasks, especially those involving long-range dependencies and complex contextual relationships."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}